{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efc51ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded: 8 bit\n",
      " GPT2LMHeadModel\n",
      "transformer GPT2Model\n",
      "transformer.wte Embedding\n",
      "transformer.wpe Embedding\n",
      "transformer.drop Dropout\n",
      "transformer.h ModuleList\n",
      "transformer.h.0 GPT2Block\n",
      "transformer.h.0.ln_1 LayerNorm\n",
      "transformer.h.0.attn GPT2Attention\n",
      "transformer.h.0.attn.c_attn Conv1D\n",
      "transformer.h.0.attn.c_proj Conv1D\n",
      "transformer.h.0.attn.attn_dropout Dropout\n",
      "transformer.h.0.attn.resid_dropout Dropout\n",
      "transformer.h.0.ln_2 LayerNorm\n",
      "transformer.h.0.mlp GPT2MLP\n",
      "transformer.h.0.mlp.c_fc Conv1D\n",
      "transformer.h.0.mlp.c_proj Conv1D\n",
      "transformer.h.0.mlp.act NewGELUActivation\n",
      "transformer.h.0.mlp.dropout Dropout\n",
      "transformer.h.1 GPT2Block\n",
      "transformer.h.1.ln_1 LayerNorm\n",
      "transformer.h.1.attn GPT2Attention\n",
      "transformer.h.1.attn.c_attn Conv1D\n",
      "transformer.h.1.attn.c_proj Conv1D\n",
      "transformer.h.1.attn.attn_dropout Dropout\n",
      "transformer.h.1.attn.resid_dropout Dropout\n",
      "transformer.h.1.ln_2 LayerNorm\n",
      "transformer.h.1.mlp GPT2MLP\n",
      "transformer.h.1.mlp.c_fc Conv1D\n",
      "transformer.h.1.mlp.c_proj Conv1D\n",
      "transformer.h.1.mlp.act NewGELUActivation\n",
      "transformer.h.1.mlp.dropout Dropout\n",
      "transformer.h.2 GPT2Block\n",
      "transformer.h.2.ln_1 LayerNorm\n",
      "transformer.h.2.attn GPT2Attention\n",
      "transformer.h.2.attn.c_attn Conv1D\n",
      "transformer.h.2.attn.c_proj Conv1D\n",
      "transformer.h.2.attn.attn_dropout Dropout\n",
      "transformer.h.2.attn.resid_dropout Dropout\n",
      "transformer.h.2.ln_2 LayerNorm\n",
      "transformer.h.2.mlp GPT2MLP\n",
      "transformer.h.2.mlp.c_fc Conv1D\n",
      "transformer.h.2.mlp.c_proj Conv1D\n",
      "transformer.h.2.mlp.act NewGELUActivation\n",
      "transformer.h.2.mlp.dropout Dropout\n",
      "transformer.h.3 GPT2Block\n",
      "transformer.h.3.ln_1 LayerNorm\n",
      "transformer.h.3.attn GPT2Attention\n",
      "transformer.h.3.attn.c_attn Conv1D\n",
      "transformer.h.3.attn.c_proj Conv1D\n",
      "transformer.h.3.attn.attn_dropout Dropout\n",
      "transformer.h.3.attn.resid_dropout Dropout\n",
      "transformer.h.3.ln_2 LayerNorm\n",
      "transformer.h.3.mlp GPT2MLP\n",
      "transformer.h.3.mlp.c_fc Conv1D\n",
      "transformer.h.3.mlp.c_proj Conv1D\n",
      "transformer.h.3.mlp.act NewGELUActivation\n",
      "transformer.h.3.mlp.dropout Dropout\n",
      "transformer.h.4 GPT2Block\n",
      "transformer.h.4.ln_1 LayerNorm\n",
      "transformer.h.4.attn GPT2Attention\n",
      "transformer.h.4.attn.c_attn Conv1D\n",
      "transformer.h.4.attn.c_proj Conv1D\n",
      "transformer.h.4.attn.attn_dropout Dropout\n",
      "transformer.h.4.attn.resid_dropout Dropout\n",
      "transformer.h.4.ln_2 LayerNorm\n",
      "transformer.h.4.mlp GPT2MLP\n",
      "transformer.h.4.mlp.c_fc Conv1D\n",
      "transformer.h.4.mlp.c_proj Conv1D\n",
      "transformer.h.4.mlp.act NewGELUActivation\n",
      "transformer.h.4.mlp.dropout Dropout\n",
      "transformer.h.5 GPT2Block\n",
      "transformer.h.5.ln_1 LayerNorm\n",
      "transformer.h.5.attn GPT2Attention\n",
      "transformer.h.5.attn.c_attn Conv1D\n",
      "transformer.h.5.attn.c_proj Conv1D\n",
      "transformer.h.5.attn.attn_dropout Dropout\n",
      "transformer.h.5.attn.resid_dropout Dropout\n",
      "transformer.h.5.ln_2 LayerNorm\n",
      "transformer.h.5.mlp GPT2MLP\n",
      "transformer.h.5.mlp.c_fc Conv1D\n",
      "transformer.h.5.mlp.c_proj Conv1D\n",
      "transformer.h.5.mlp.act NewGELUActivation\n",
      "transformer.h.5.mlp.dropout Dropout\n",
      "transformer.h.6 GPT2Block\n",
      "transformer.h.6.ln_1 LayerNorm\n",
      "transformer.h.6.attn GPT2Attention\n",
      "transformer.h.6.attn.c_attn Conv1D\n",
      "transformer.h.6.attn.c_proj Conv1D\n",
      "transformer.h.6.attn.attn_dropout Dropout\n",
      "transformer.h.6.attn.resid_dropout Dropout\n",
      "transformer.h.6.ln_2 LayerNorm\n",
      "transformer.h.6.mlp GPT2MLP\n",
      "transformer.h.6.mlp.c_fc Conv1D\n",
      "transformer.h.6.mlp.c_proj Conv1D\n",
      "transformer.h.6.mlp.act NewGELUActivation\n",
      "transformer.h.6.mlp.dropout Dropout\n",
      "transformer.h.7 GPT2Block\n",
      "transformer.h.7.ln_1 LayerNorm\n",
      "transformer.h.7.attn GPT2Attention\n",
      "transformer.h.7.attn.c_attn Conv1D\n",
      "transformer.h.7.attn.c_proj Conv1D\n",
      "transformer.h.7.attn.attn_dropout Dropout\n",
      "transformer.h.7.attn.resid_dropout Dropout\n",
      "transformer.h.7.ln_2 LayerNorm\n",
      "transformer.h.7.mlp GPT2MLP\n",
      "transformer.h.7.mlp.c_fc Conv1D\n",
      "transformer.h.7.mlp.c_proj Conv1D\n",
      "transformer.h.7.mlp.act NewGELUActivation\n",
      "transformer.h.7.mlp.dropout Dropout\n",
      "transformer.h.8 GPT2Block\n",
      "transformer.h.8.ln_1 LayerNorm\n",
      "transformer.h.8.attn GPT2Attention\n",
      "transformer.h.8.attn.c_attn Conv1D\n",
      "transformer.h.8.attn.c_proj Conv1D\n",
      "transformer.h.8.attn.attn_dropout Dropout\n",
      "transformer.h.8.attn.resid_dropout Dropout\n",
      "transformer.h.8.ln_2 LayerNorm\n",
      "transformer.h.8.mlp GPT2MLP\n",
      "transformer.h.8.mlp.c_fc Conv1D\n",
      "transformer.h.8.mlp.c_proj Conv1D\n",
      "transformer.h.8.mlp.act NewGELUActivation\n",
      "transformer.h.8.mlp.dropout Dropout\n",
      "transformer.h.9 GPT2Block\n",
      "transformer.h.9.ln_1 LayerNorm\n",
      "transformer.h.9.attn GPT2Attention\n",
      "transformer.h.9.attn.c_attn Conv1D\n",
      "transformer.h.9.attn.c_proj Conv1D\n",
      "transformer.h.9.attn.attn_dropout Dropout\n",
      "transformer.h.9.attn.resid_dropout Dropout\n",
      "transformer.h.9.ln_2 LayerNorm\n",
      "transformer.h.9.mlp GPT2MLP\n",
      "transformer.h.9.mlp.c_fc Conv1D\n",
      "transformer.h.9.mlp.c_proj Conv1D\n",
      "transformer.h.9.mlp.act NewGELUActivation\n",
      "transformer.h.9.mlp.dropout Dropout\n",
      "transformer.h.10 GPT2Block\n",
      "transformer.h.10.ln_1 LayerNorm\n",
      "transformer.h.10.attn GPT2Attention\n",
      "transformer.h.10.attn.c_attn Conv1D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.10.attn.c_proj Conv1D\n",
      "transformer.h.10.attn.attn_dropout Dropout\n",
      "transformer.h.10.attn.resid_dropout Dropout\n",
      "transformer.h.10.ln_2 LayerNorm\n",
      "transformer.h.10.mlp GPT2MLP\n",
      "transformer.h.10.mlp.c_fc Conv1D\n",
      "transformer.h.10.mlp.c_proj Conv1D\n",
      "transformer.h.10.mlp.act NewGELUActivation\n",
      "transformer.h.10.mlp.dropout Dropout\n",
      "transformer.h.11 GPT2Block\n",
      "transformer.h.11.ln_1 LayerNorm\n",
      "transformer.h.11.attn GPT2Attention\n",
      "transformer.h.11.attn.c_attn Conv1D\n",
      "transformer.h.11.attn.c_proj Conv1D\n",
      "transformer.h.11.attn.attn_dropout Dropout\n",
      "transformer.h.11.attn.resid_dropout Dropout\n",
      "transformer.h.11.ln_2 LayerNorm\n",
      "transformer.h.11.mlp GPT2MLP\n",
      "transformer.h.11.mlp.c_fc Conv1D\n",
      "transformer.h.11.mlp.c_proj Conv1D\n",
      "transformer.h.11.mlp.act NewGELUActivation\n",
      "transformer.h.11.mlp.dropout Dropout\n",
      "transformer.ln_f LayerNorm\n",
      "lm_head Linear\n",
      "✅ Model quantized successfully\n",
      "✅ Generation test: Hello, world is  going to be a lot more interesting than it was before.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# Step 1: GPT-2 quantization migration test\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from src.quantization import QuantLinear, replace_with_quant, load_config\n",
    "\n",
    "# Load model and config\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "cfg = load_config(\"../configs/config.yaml\")\n",
    "print(f\"✅ Config loaded: {cfg['default_w_bits']} bit\")\n",
    "\n",
    "# Execute quantization replacement (keep layer printing)\n",
    "replace_with_quant(model, cfg)\n",
    "print(\"✅ Model quantized successfully\")\n",
    "\n",
    "# Test basic generation\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=20)\n",
    "print(\"✅ Generation test:\", tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0733754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully quantized 48 layers\n",
      "\n",
      "--- Generation test ---\n",
      "Result: Hello, world is  in the world. \n",
      "I've been in the a small small small small small...\n",
      "\n",
      "✅ Step 1 completed!\n"
     ]
    }
   ],
   "source": [
    "# Complete Step 1 functionality validation\n",
    "from src.quantization import requantize_model_to_config\n",
    "\n",
    "# Count quantized layers\n",
    "quant_count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, QuantLinear):\n",
    "        quant_count += 1\n",
    "print(f\"✅ Successfully quantized {quant_count} layers\")\n",
    "\n",
    "# Test 2bit/6bit dynamic requantization\n",
    "test_cfg = load_config(\"../configs/test_2bit_6bit.yaml\")\n",
    "requantize_model_to_config(model, test_cfg)\n",
    "\n",
    "# Test generation with quantized model\n",
    "print(\"\\n--- Generation test ---\")\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(f\"Result: {result[:80]}...\")\n",
    "\n",
    "print(\"\\n✅ Step 1 completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b17f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT2LMHeadModel\n",
      "transformer GPT2Model\n",
      "transformer.wte Embedding\n",
      "transformer.wpe Embedding\n",
      "transformer.drop Dropout\n",
      "transformer.h ModuleList\n",
      "transformer.h.0 GPT2Block\n",
      "transformer.h.0.ln_1 LayerNorm\n",
      "transformer.h.0.attn GPT2Attention\n",
      "transformer.h.0.attn.c_attn Conv1D\n",
      "transformer.h.0.attn.c_proj Conv1D\n",
      "transformer.h.0.attn.attn_dropout Dropout\n",
      "transformer.h.0.attn.resid_dropout Dropout\n",
      "transformer.h.0.ln_2 LayerNorm\n",
      "transformer.h.0.mlp GPT2MLP\n",
      "transformer.h.0.mlp.c_fc Conv1D\n",
      "transformer.h.0.mlp.c_proj Conv1D\n",
      "transformer.h.0.mlp.act NewGELUActivation\n",
      "transformer.h.0.mlp.dropout Dropout\n",
      "transformer.h.1 GPT2Block\n",
      "transformer.h.1.ln_1 LayerNorm\n",
      "transformer.h.1.attn GPT2Attention\n",
      "transformer.h.1.attn.c_attn Conv1D\n",
      "transformer.h.1.attn.c_proj Conv1D\n",
      "transformer.h.1.attn.attn_dropout Dropout\n",
      "transformer.h.1.attn.resid_dropout Dropout\n",
      "transformer.h.1.ln_2 LayerNorm\n",
      "transformer.h.1.mlp GPT2MLP\n",
      "transformer.h.1.mlp.c_fc Conv1D\n",
      "transformer.h.1.mlp.c_proj Conv1D\n",
      "transformer.h.1.mlp.act NewGELUActivation\n",
      "transformer.h.1.mlp.dropout Dropout\n",
      "transformer.h.2 GPT2Block\n",
      "transformer.h.2.ln_1 LayerNorm\n",
      "transformer.h.2.attn GPT2Attention\n",
      "transformer.h.2.attn.c_attn Conv1D\n",
      "transformer.h.2.attn.c_proj Conv1D\n",
      "transformer.h.2.attn.attn_dropout Dropout\n",
      "transformer.h.2.attn.resid_dropout Dropout\n",
      "transformer.h.2.ln_2 LayerNorm\n",
      "transformer.h.2.mlp GPT2MLP\n",
      "transformer.h.2.mlp.c_fc Conv1D\n",
      "transformer.h.2.mlp.c_proj Conv1D\n",
      "transformer.h.2.mlp.act NewGELUActivation\n",
      "transformer.h.2.mlp.dropout Dropout\n",
      "transformer.h.3 GPT2Block\n",
      "transformer.h.3.ln_1 LayerNorm\n",
      "transformer.h.3.attn GPT2Attention\n",
      "transformer.h.3.attn.c_attn Conv1D\n",
      "transformer.h.3.attn.c_proj Conv1D\n",
      "transformer.h.3.attn.attn_dropout Dropout\n",
      "transformer.h.3.attn.resid_dropout Dropout\n",
      "transformer.h.3.ln_2 LayerNorm\n",
      "transformer.h.3.mlp GPT2MLP\n",
      "transformer.h.3.mlp.c_fc Conv1D\n",
      "transformer.h.3.mlp.c_proj Conv1D\n",
      "transformer.h.3.mlp.act NewGELUActivation\n",
      "transformer.h.3.mlp.dropout Dropout\n",
      "transformer.h.4 GPT2Block\n",
      "transformer.h.4.ln_1 LayerNorm\n",
      "transformer.h.4.attn GPT2Attention\n",
      "transformer.h.4.attn.c_attn Conv1D\n",
      "transformer.h.4.attn.c_proj Conv1D\n",
      "transformer.h.4.attn.attn_dropout Dropout\n",
      "transformer.h.4.attn.resid_dropout Dropout\n",
      "transformer.h.4.ln_2 LayerNorm\n",
      "transformer.h.4.mlp GPT2MLP\n",
      "transformer.h.4.mlp.c_fc Conv1D\n",
      "transformer.h.4.mlp.c_proj Conv1D\n",
      "transformer.h.4.mlp.act NewGELUActivation\n",
      "transformer.h.4.mlp.dropout Dropout\n",
      "transformer.h.5 GPT2Block\n",
      "transformer.h.5.ln_1 LayerNorm\n",
      "transformer.h.5.attn GPT2Attention\n",
      "transformer.h.5.attn.c_attn Conv1D\n",
      "transformer.h.5.attn.c_proj Conv1D\n",
      "transformer.h.5.attn.attn_dropout Dropout\n",
      "transformer.h.5.attn.resid_dropout Dropout\n",
      "transformer.h.5.ln_2 LayerNorm\n",
      "transformer.h.5.mlp GPT2MLP\n",
      "transformer.h.5.mlp.c_fc Conv1D\n",
      "transformer.h.5.mlp.c_proj Conv1D\n",
      "transformer.h.5.mlp.act NewGELUActivation\n",
      "transformer.h.5.mlp.dropout Dropout\n",
      "transformer.h.6 GPT2Block\n",
      "transformer.h.6.ln_1 LayerNorm\n",
      "transformer.h.6.attn GPT2Attention\n",
      "transformer.h.6.attn.c_attn Conv1D\n",
      "transformer.h.6.attn.c_proj Conv1D\n",
      "transformer.h.6.attn.attn_dropout Dropout\n",
      "transformer.h.6.attn.resid_dropout Dropout\n",
      "transformer.h.6.ln_2 LayerNorm\n",
      "transformer.h.6.mlp GPT2MLP\n",
      "transformer.h.6.mlp.c_fc Conv1D\n",
      "transformer.h.6.mlp.c_proj Conv1D\n",
      "transformer.h.6.mlp.act NewGELUActivation\n",
      "transformer.h.6.mlp.dropout Dropout\n",
      "transformer.h.7 GPT2Block\n",
      "transformer.h.7.ln_1 LayerNorm\n",
      "transformer.h.7.attn GPT2Attention\n",
      "transformer.h.7.attn.c_attn Conv1D\n",
      "transformer.h.7.attn.c_proj Conv1D\n",
      "transformer.h.7.attn.attn_dropout Dropout\n",
      "transformer.h.7.attn.resid_dropout Dropout\n",
      "transformer.h.7.ln_2 LayerNorm\n",
      "transformer.h.7.mlp GPT2MLP\n",
      "transformer.h.7.mlp.c_fc Conv1D\n",
      "transformer.h.7.mlp.c_proj Conv1D\n",
      "transformer.h.7.mlp.act NewGELUActivation\n",
      "transformer.h.7.mlp.dropout Dropout\n",
      "transformer.h.8 GPT2Block\n",
      "transformer.h.8.ln_1 LayerNorm\n",
      "transformer.h.8.attn GPT2Attention\n",
      "transformer.h.8.attn.c_attn Conv1D\n",
      "transformer.h.8.attn.c_proj Conv1D\n",
      "transformer.h.8.attn.attn_dropout Dropout\n",
      "transformer.h.8.attn.resid_dropout Dropout\n",
      "transformer.h.8.ln_2 LayerNorm\n",
      "transformer.h.8.mlp GPT2MLP\n",
      "transformer.h.8.mlp.c_fc Conv1D\n",
      "transformer.h.8.mlp.c_proj Conv1D\n",
      "transformer.h.8.mlp.act NewGELUActivation\n",
      "transformer.h.8.mlp.dropout Dropout\n",
      "transformer.h.9 GPT2Block\n",
      "transformer.h.9.ln_1 LayerNorm\n",
      "transformer.h.9.attn GPT2Attention\n",
      "transformer.h.9.attn.c_attn Conv1D\n",
      "transformer.h.9.attn.c_proj Conv1D\n",
      "transformer.h.9.attn.attn_dropout Dropout\n",
      "transformer.h.9.attn.resid_dropout Dropout\n",
      "transformer.h.9.ln_2 LayerNorm\n",
      "transformer.h.9.mlp GPT2MLP\n",
      "transformer.h.9.mlp.c_fc Conv1D\n",
      "transformer.h.9.mlp.c_proj Conv1D\n",
      "transformer.h.9.mlp.act NewGELUActivation\n",
      "transformer.h.9.mlp.dropout Dropout\n",
      "transformer.h.10 GPT2Block\n",
      "transformer.h.10.ln_1 LayerNorm\n",
      "transformer.h.10.attn GPT2Attention\n",
      "transformer.h.10.attn.c_attn Conv1D\n",
      "transformer.h.10.attn.c_proj Conv1D\n",
      "transformer.h.10.attn.attn_dropout Dropout\n",
      "transformer.h.10.attn.resid_dropout Dropout\n",
      "transformer.h.10.ln_2 LayerNorm\n",
      "transformer.h.10.mlp GPT2MLP\n",
      "transformer.h.10.mlp.c_fc Conv1D\n",
      "transformer.h.10.mlp.c_proj Conv1D\n",
      "transformer.h.10.mlp.act NewGELUActivation\n",
      "transformer.h.10.mlp.dropout Dropout\n",
      "transformer.h.11 GPT2Block\n",
      "transformer.h.11.ln_1 LayerNorm\n",
      "transformer.h.11.attn GPT2Attention\n",
      "transformer.h.11.attn.c_attn Conv1D\n",
      "transformer.h.11.attn.c_proj Conv1D\n",
      "transformer.h.11.attn.attn_dropout Dropout\n",
      "transformer.h.11.attn.resid_dropout Dropout\n",
      "transformer.h.11.ln_2 LayerNorm\n",
      "transformer.h.11.mlp GPT2MLP\n",
      "transformer.h.11.mlp.c_fc Conv1D\n",
      "transformer.h.11.mlp.c_proj Conv1D\n",
      "transformer.h.11.mlp.act NewGELUActivation\n",
      "transformer.h.11.mlp.dropout Dropout\n",
      "transformer.ln_f LayerNorm\n",
      "lm_head Linear\n",
      "Warning: LoRA branch 'bw999' not found in transformer.h.0.attn.c_attn, using None\n",
      "Sampled:  going to be a lot more interesting than it was before.\n",
      "I'm not sure if I'm goin...\n",
      "✅ Step 2 completed: 2 wrappers, 12 params\n"
     ]
    }
   ],
   "source": [
    "# Step 2: LoRA migration test\n",
    "from src.lora import attach_lora_to_quant, activate_lora_by_config, activate_lora_by_bits\n",
    "\n",
    "# Set same random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# LoRA spec with 6-bit support (same as test1)\n",
    "lora_spec = {\n",
    "    \"transformer.h.0.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8), \"bw6\": (6,12)},\n",
    "    \"transformer.h.1.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8), \"bw6\": (6,12)},\n",
    "}\n",
    "\n",
    "# Fresh model + quantization + LoRA\n",
    "lora_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "wrappers = attach_lora_to_quant(lora_model, lora_spec, cfg)\n",
    "\n",
    "# Test 6-bit activation and fallback\n",
    "test_6bit_cfg = {\"per_layer_bits\": {\"transformer.h.0.attn.c_attn\": 6}, \"default_w_bits\": 8}\n",
    "activate_lora_by_config(wrappers, test_6bit_cfg)\n",
    "\n",
    "# Test branch switching and parameter collection\n",
    "test_wrapper = list(wrappers.values())[0]\n",
    "test_wrapper.set_active(\"bw4\")\n",
    "test_wrapper.set_active(\"bw999\")  # trigger fallback warning\n",
    "\n",
    "lora_params = [p for w in wrappers.values() for p in w.bank.parameters() if p.requires_grad]\n",
    "\n",
    "# Set consistent activation state for generation test\n",
    "for wrapper in wrappers.values():\n",
    "    wrapper.set_active(\"bw8\")  # ensure same activation state\n",
    "\n",
    "# Deterministic generation test (minimal)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "lora_model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inp = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = lora_model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "res = tokenizer.decode(out[0, inp[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "print(\"Sampled:\", res[:80] + \"...\")\n",
    "\n",
    "print(f\"✅ Step 2 completed: {len(wrappers)} wrappers, {len(lora_params)} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58596257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Iteration 1000 | Loss: 4.004 | Config: 4-bit: 100%|██████████| 1000/1000 [00:42<00:00, 23.72it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "After training: Hello, world is  going to be a lot more interesting than it was before.\n",
      "I'm not ...\n",
      "✅ Step 3 completed: switchable training with 2 configs\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Switchable precision training test\n",
    "from src.training import create_squad_dataloader, SwitchableTrainer\n",
    "from src.quantization import load_config\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepare multiple configs for switchable training\n",
    "config_A = load_config(\"../configs/config.yaml\")\n",
    "config_B = load_config(\"../configs/config_4bit.yaml\")\n",
    "precision_configs = [config_A, config_B]\n",
    "\n",
    "# Create dataloader and trainer\n",
    "train_dataloader = create_squad_dataloader(tokenizer, batch_size=4, subset_size=100)\n",
    "trainer = SwitchableTrainer(lora_model, wrappers, precision_configs, lr=1e-4)\n",
    "\n",
    "# Run training (1000 iterations)\n",
    "trainer.train(train_dataloader, iterations=1000)\n",
    "\n",
    "# Test generation after training (CUDA-only)\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"CUDA is required for generation but not found.\"\n",
    "\n",
    "lora_model.eval().to(\"cuda\")\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model.generate(**inputs, max_length=50)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(\"After training:\", result[:80] + \"...\")\n",
    "\n",
    "print(f\"✅ Step 3 completed: switchable training with {len(precision_configs)} configs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            config  EM       F1  default_bits\n",
      "           C1_all8 0.0 9.125855             8\n",
      "            config 0.0 9.125855             8\n",
      "    test_2bit_6bit 0.0 6.792441             4\n",
      "   C4_back8_front4 0.0 5.996765             4\n",
      "C8_mlpfc4_mlpproj8 0.0 5.515476             4\n",
      "           C2_all4 0.0 4.583590             4\n",
      " C9_layernorm_fp32 0.0 4.583590             4\n",
      "       config_4bit 0.0 4.583590             4\n",
      "     C7_qkv4_proj8 0.0 4.418269             4\n",
      "   C3_front8_back4 0.0 4.096109             4\n",
      "  C10_mixed_budget 0.0 4.048412             4\n",
      "     C6_qkv8_proj4 0.0 3.658520             4\n",
      "       C5_sandwich 0.0 3.054139             4\n",
      "Results saved to step4_results.json\n",
      "✅ Step 4 completed: evaluated 13 configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluation test using src modules\n",
    "from src.evaluation import analyze_all_configs, save_results\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load validation dataset\n",
    "val_ds = load_dataset(\"squad\", split=\"validation\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Analyze configs using migrated functions\n",
    "df = analyze_all_configs(lora_model, tokenizer, val_ds, device, n=50)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"✅ Step 4 completed: evaluated {len(df)} configs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B: CPT (probe) vs JOINT (parallel) on GPU only for step 5\n",
    "import logging, torch\n",
    "from time import perf_counter\n",
    "from scripts.step5_cpt_slope import build_toy_mlp, train\n",
    "from src.quantization.model_utils import replace_with_quant\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA required\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "device = \"cuda\"\n",
    "steps = 300\n",
    "bitwidths = [8, 6, 4]     # highest first\n",
    "segment_steps = 60\n",
    "probe_steps = 8\n",
    "bs, inp, hid, cls = 32, 128, 64, 10\n",
    "seed = 42\n",
    "\n",
    "def run_once(mode: str):\n",
    "    model = build_toy_mlp(inp=inp, hid=hid, cls=cls)\n",
    "    ret = replace_with_quant(model, {\"default_w_bits\": bitwidths[0], \"per_layer_bits\": {}})\n",
    "    if ret is not None:\n",
    "        model = ret\n",
    "    t0 = perf_counter()\n",
    "    hist = train(model=model, steps=steps, bitwidths=bitwidths, mode=mode,\n",
    "                 segment_steps=segment_steps, probe_steps=probe_steps,\n",
    "                 bs=bs, inp=inp, cls=cls, dev=device, lr=1e-3, seed=seed)\n",
    "    dt = perf_counter() - t0\n",
    "    return hist, dt\n",
    "\n",
    "def summarize(hist):\n",
    "    tail = hist[-20:] if len(hist) >= 20 else hist\n",
    "    avg_last = sum(l for _, l in tail) / max(1, len(tail))\n",
    "    bits = [b for b, _ in hist]\n",
    "    dist = {}\n",
    "    for b in bits:\n",
    "        dist[b] = dist.get(b, 0) + 1\n",
    "    pure = [b for b in bits if b != -1]\n",
    "    switches = sum(1 for i in range(1, len(pure)) if pure[i] != pure[i-1]) if pure else 0\n",
    "    return avg_last, dist, switches\n",
    "\n",
    "hist_cpt, dt_cpt = run_once(\"cpt\")\n",
    "avg_cpt, dist_cpt, sw_cpt = summarize(hist_cpt)\n",
    "\n",
    "hist_joint, dt_joint = run_once(\"joint\")\n",
    "avg_joint, dist_joint, _ = summarize(hist_joint)\n",
    "\n",
    "print(f\"device={device}\")\n",
    "print(f\"CPT:   steps={len(hist_cpt)} time={dt_cpt:.2f}s avg_last_loss={avg_cpt:.4f} switches={sw_cpt} dist={dist_cpt}\")\n",
    "print(f\"JOINT: steps={len(hist_joint)} time={dt_joint:.2f}s avg_last_loss={avg_joint:.4f} dist={dist_joint}\")\n",
    "print(\"CPT last 5:\", hist_cpt[-5:])\n",
    "print(\"JOINT last 5:\", hist_joint[-5:])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
