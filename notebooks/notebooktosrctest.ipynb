{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0efc51ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config loaded: 8 bit\n",
      " GPT2LMHeadModel\n",
      "transformer GPT2Model\n",
      "transformer.wte Embedding\n",
      "transformer.wpe Embedding\n",
      "transformer.drop Dropout\n",
      "transformer.h ModuleList\n",
      "transformer.h.0 GPT2Block\n",
      "transformer.h.0.ln_1 LayerNorm\n",
      "transformer.h.0.attn GPT2Attention\n",
      "transformer.h.0.attn.c_attn Conv1D\n",
      "transformer.h.0.attn.c_proj Conv1D\n",
      "transformer.h.0.attn.attn_dropout Dropout\n",
      "transformer.h.0.attn.resid_dropout Dropout\n",
      "transformer.h.0.ln_2 LayerNorm\n",
      "transformer.h.0.mlp GPT2MLP\n",
      "transformer.h.0.mlp.c_fc Conv1D\n",
      "transformer.h.0.mlp.c_proj Conv1D\n",
      "transformer.h.0.mlp.act NewGELUActivation\n",
      "transformer.h.0.mlp.dropout Dropout\n",
      "transformer.h.1 GPT2Block\n",
      "transformer.h.1.ln_1 LayerNorm\n",
      "transformer.h.1.attn GPT2Attention\n",
      "transformer.h.1.attn.c_attn Conv1D\n",
      "transformer.h.1.attn.c_proj Conv1D\n",
      "transformer.h.1.attn.attn_dropout Dropout\n",
      "transformer.h.1.attn.resid_dropout Dropout\n",
      "transformer.h.1.ln_2 LayerNorm\n",
      "transformer.h.1.mlp GPT2MLP\n",
      "transformer.h.1.mlp.c_fc Conv1D\n",
      "transformer.h.1.mlp.c_proj Conv1D\n",
      "transformer.h.1.mlp.act NewGELUActivation\n",
      "transformer.h.1.mlp.dropout Dropout\n",
      "transformer.h.2 GPT2Block\n",
      "transformer.h.2.ln_1 LayerNorm\n",
      "transformer.h.2.attn GPT2Attention\n",
      "transformer.h.2.attn.c_attn Conv1D\n",
      "transformer.h.2.attn.c_proj Conv1D\n",
      "transformer.h.2.attn.attn_dropout Dropout\n",
      "transformer.h.2.attn.resid_dropout Dropout\n",
      "transformer.h.2.ln_2 LayerNorm\n",
      "transformer.h.2.mlp GPT2MLP\n",
      "transformer.h.2.mlp.c_fc Conv1D\n",
      "transformer.h.2.mlp.c_proj Conv1D\n",
      "transformer.h.2.mlp.act NewGELUActivation\n",
      "transformer.h.2.mlp.dropout Dropout\n",
      "transformer.h.3 GPT2Block\n",
      "transformer.h.3.ln_1 LayerNorm\n",
      "transformer.h.3.attn GPT2Attention\n",
      "transformer.h.3.attn.c_attn Conv1D\n",
      "transformer.h.3.attn.c_proj Conv1D\n",
      "transformer.h.3.attn.attn_dropout Dropout\n",
      "transformer.h.3.attn.resid_dropout Dropout\n",
      "transformer.h.3.ln_2 LayerNorm\n",
      "transformer.h.3.mlp GPT2MLP\n",
      "transformer.h.3.mlp.c_fc Conv1D\n",
      "transformer.h.3.mlp.c_proj Conv1D\n",
      "transformer.h.3.mlp.act NewGELUActivation\n",
      "transformer.h.3.mlp.dropout Dropout\n",
      "transformer.h.4 GPT2Block\n",
      "transformer.h.4.ln_1 LayerNorm\n",
      "transformer.h.4.attn GPT2Attention\n",
      "transformer.h.4.attn.c_attn Conv1D\n",
      "transformer.h.4.attn.c_proj Conv1D\n",
      "transformer.h.4.attn.attn_dropout Dropout\n",
      "transformer.h.4.attn.resid_dropout Dropout\n",
      "transformer.h.4.ln_2 LayerNorm\n",
      "transformer.h.4.mlp GPT2MLP\n",
      "transformer.h.4.mlp.c_fc Conv1D\n",
      "transformer.h.4.mlp.c_proj Conv1D\n",
      "transformer.h.4.mlp.act NewGELUActivation\n",
      "transformer.h.4.mlp.dropout Dropout\n",
      "transformer.h.5 GPT2Block\n",
      "transformer.h.5.ln_1 LayerNorm\n",
      "transformer.h.5.attn GPT2Attention\n",
      "transformer.h.5.attn.c_attn Conv1D\n",
      "transformer.h.5.attn.c_proj Conv1D\n",
      "transformer.h.5.attn.attn_dropout Dropout\n",
      "transformer.h.5.attn.resid_dropout Dropout\n",
      "transformer.h.5.ln_2 LayerNorm\n",
      "transformer.h.5.mlp GPT2MLP\n",
      "transformer.h.5.mlp.c_fc Conv1D\n",
      "transformer.h.5.mlp.c_proj Conv1D\n",
      "transformer.h.5.mlp.act NewGELUActivation\n",
      "transformer.h.5.mlp.dropout Dropout\n",
      "transformer.h.6 GPT2Block\n",
      "transformer.h.6.ln_1 LayerNorm\n",
      "transformer.h.6.attn GPT2Attention\n",
      "transformer.h.6.attn.c_attn Conv1D\n",
      "transformer.h.6.attn.c_proj Conv1D\n",
      "transformer.h.6.attn.attn_dropout Dropout\n",
      "transformer.h.6.attn.resid_dropout Dropout\n",
      "transformer.h.6.ln_2 LayerNorm\n",
      "transformer.h.6.mlp GPT2MLP\n",
      "transformer.h.6.mlp.c_fc Conv1D\n",
      "transformer.h.6.mlp.c_proj Conv1D\n",
      "transformer.h.6.mlp.act NewGELUActivation\n",
      "transformer.h.6.mlp.dropout Dropout\n",
      "transformer.h.7 GPT2Block\n",
      "transformer.h.7.ln_1 LayerNorm\n",
      "transformer.h.7.attn GPT2Attention\n",
      "transformer.h.7.attn.c_attn Conv1D\n",
      "transformer.h.7.attn.c_proj Conv1D\n",
      "transformer.h.7.attn.attn_dropout Dropout\n",
      "transformer.h.7.attn.resid_dropout Dropout\n",
      "transformer.h.7.ln_2 LayerNorm\n",
      "transformer.h.7.mlp GPT2MLP\n",
      "transformer.h.7.mlp.c_fc Conv1D\n",
      "transformer.h.7.mlp.c_proj Conv1D\n",
      "transformer.h.7.mlp.act NewGELUActivation\n",
      "transformer.h.7.mlp.dropout Dropout\n",
      "transformer.h.8 GPT2Block\n",
      "transformer.h.8.ln_1 LayerNorm\n",
      "transformer.h.8.attn GPT2Attention\n",
      "transformer.h.8.attn.c_attn Conv1D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.8.attn.c_proj Conv1D\n",
      "transformer.h.8.attn.attn_dropout Dropout\n",
      "transformer.h.8.attn.resid_dropout Dropout\n",
      "transformer.h.8.ln_2 LayerNorm\n",
      "transformer.h.8.mlp GPT2MLP\n",
      "transformer.h.8.mlp.c_fc Conv1D\n",
      "transformer.h.8.mlp.c_proj Conv1D\n",
      "transformer.h.8.mlp.act NewGELUActivation\n",
      "transformer.h.8.mlp.dropout Dropout\n",
      "transformer.h.9 GPT2Block\n",
      "transformer.h.9.ln_1 LayerNorm\n",
      "transformer.h.9.attn GPT2Attention\n",
      "transformer.h.9.attn.c_attn Conv1D\n",
      "transformer.h.9.attn.c_proj Conv1D\n",
      "transformer.h.9.attn.attn_dropout Dropout\n",
      "transformer.h.9.attn.resid_dropout Dropout\n",
      "transformer.h.9.ln_2 LayerNorm\n",
      "transformer.h.9.mlp GPT2MLP\n",
      "transformer.h.9.mlp.c_fc Conv1D\n",
      "transformer.h.9.mlp.c_proj Conv1D\n",
      "transformer.h.9.mlp.act NewGELUActivation\n",
      "transformer.h.9.mlp.dropout Dropout\n",
      "transformer.h.10 GPT2Block\n",
      "transformer.h.10.ln_1 LayerNorm\n",
      "transformer.h.10.attn GPT2Attention\n",
      "transformer.h.10.attn.c_attn Conv1D\n",
      "transformer.h.10.attn.c_proj Conv1D\n",
      "transformer.h.10.attn.attn_dropout Dropout\n",
      "transformer.h.10.attn.resid_dropout Dropout\n",
      "transformer.h.10.ln_2 LayerNorm\n",
      "transformer.h.10.mlp GPT2MLP\n",
      "transformer.h.10.mlp.c_fc Conv1D\n",
      "transformer.h.10.mlp.c_proj Conv1D\n",
      "transformer.h.10.mlp.act NewGELUActivation\n",
      "transformer.h.10.mlp.dropout Dropout\n",
      "transformer.h.11 GPT2Block\n",
      "transformer.h.11.ln_1 LayerNorm\n",
      "transformer.h.11.attn GPT2Attention\n",
      "transformer.h.11.attn.c_attn Conv1D\n",
      "transformer.h.11.attn.c_proj Conv1D\n",
      "transformer.h.11.attn.attn_dropout Dropout\n",
      "transformer.h.11.attn.resid_dropout Dropout\n",
      "transformer.h.11.ln_2 LayerNorm\n",
      "transformer.h.11.mlp GPT2MLP\n",
      "transformer.h.11.mlp.c_fc Conv1D\n",
      "transformer.h.11.mlp.c_proj Conv1D\n",
      "transformer.h.11.mlp.act NewGELUActivation\n",
      "transformer.h.11.mlp.dropout Dropout\n",
      "transformer.ln_f LayerNorm\n",
      "lm_head Linear\n",
      "✅ Model quantized successfully\n",
      "✅ Generation test: Hello, world is  going to be a lot more interesting than it was before.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# Step 1: GPT-2 quantization migration test\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from src.quantization import QuantLinear, replace_with_quant, load_config\n",
    "\n",
    "# Load model and config\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "cfg = load_config(\"../configs/config.yaml\")\n",
    "print(f\"✅ Config loaded: {cfg['default_w_bits']} bit\")\n",
    "\n",
    "# Execute quantization replacement (keep layer printing)\n",
    "replace_with_quant(model, cfg)\n",
    "print(\"✅ Model quantized successfully\")\n",
    "\n",
    "# Test basic generation\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=20)\n",
    "print(\"✅ Generation test:\", tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0733754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully quantized 48 layers\n",
      "\n",
      "--- Generation test ---\n",
      "Result: Hello, world is  in the world. \n",
      "I've been in the a small small small small small...\n",
      "\n",
      "✅ Step 1 completed!\n"
     ]
    }
   ],
   "source": [
    "# Complete Step 1 functionality validation\n",
    "from src.quantization import requantize_model_to_config\n",
    "\n",
    "# Count quantized layers\n",
    "quant_count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, QuantLinear):\n",
    "        quant_count += 1\n",
    "print(f\"✅ Successfully quantized {quant_count} layers\")\n",
    "\n",
    "# Test 2bit/6bit dynamic requantization\n",
    "test_cfg = load_config(\"../configs/test_2bit_6bit.yaml\")\n",
    "requantize_model_to_config(model, test_cfg)\n",
    "\n",
    "# Test generation with quantized model\n",
    "print(\"\\n--- Generation test ---\")\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(f\"Result: {result[:80]}...\")\n",
    "\n",
    "print(\"\\n✅ Step 1 completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5b17f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT2LMHeadModel\n",
      "transformer GPT2Model\n",
      "transformer.wte Embedding\n",
      "transformer.wpe Embedding\n",
      "transformer.drop Dropout\n",
      "transformer.h ModuleList\n",
      "transformer.h.0 GPT2Block\n",
      "transformer.h.0.ln_1 LayerNorm\n",
      "transformer.h.0.attn GPT2Attention\n",
      "transformer.h.0.attn.c_attn Conv1D\n",
      "transformer.h.0.attn.c_proj Conv1D\n",
      "transformer.h.0.attn.attn_dropout Dropout\n",
      "transformer.h.0.attn.resid_dropout Dropout\n",
      "transformer.h.0.ln_2 LayerNorm\n",
      "transformer.h.0.mlp GPT2MLP\n",
      "transformer.h.0.mlp.c_fc Conv1D\n",
      "transformer.h.0.mlp.c_proj Conv1D\n",
      "transformer.h.0.mlp.act NewGELUActivation\n",
      "transformer.h.0.mlp.dropout Dropout\n",
      "transformer.h.1 GPT2Block\n",
      "transformer.h.1.ln_1 LayerNorm\n",
      "transformer.h.1.attn GPT2Attention\n",
      "transformer.h.1.attn.c_attn Conv1D\n",
      "transformer.h.1.attn.c_proj Conv1D\n",
      "transformer.h.1.attn.attn_dropout Dropout\n",
      "transformer.h.1.attn.resid_dropout Dropout\n",
      "transformer.h.1.ln_2 LayerNorm\n",
      "transformer.h.1.mlp GPT2MLP\n",
      "transformer.h.1.mlp.c_fc Conv1D\n",
      "transformer.h.1.mlp.c_proj Conv1D\n",
      "transformer.h.1.mlp.act NewGELUActivation\n",
      "transformer.h.1.mlp.dropout Dropout\n",
      "transformer.h.2 GPT2Block\n",
      "transformer.h.2.ln_1 LayerNorm\n",
      "transformer.h.2.attn GPT2Attention\n",
      "transformer.h.2.attn.c_attn Conv1D\n",
      "transformer.h.2.attn.c_proj Conv1D\n",
      "transformer.h.2.attn.attn_dropout Dropout\n",
      "transformer.h.2.attn.resid_dropout Dropout\n",
      "transformer.h.2.ln_2 LayerNorm\n",
      "transformer.h.2.mlp GPT2MLP\n",
      "transformer.h.2.mlp.c_fc Conv1D\n",
      "transformer.h.2.mlp.c_proj Conv1D\n",
      "transformer.h.2.mlp.act NewGELUActivation\n",
      "transformer.h.2.mlp.dropout Dropout\n",
      "transformer.h.3 GPT2Block\n",
      "transformer.h.3.ln_1 LayerNorm\n",
      "transformer.h.3.attn GPT2Attention\n",
      "transformer.h.3.attn.c_attn Conv1D\n",
      "transformer.h.3.attn.c_proj Conv1D\n",
      "transformer.h.3.attn.attn_dropout Dropout\n",
      "transformer.h.3.attn.resid_dropout Dropout\n",
      "transformer.h.3.ln_2 LayerNorm\n",
      "transformer.h.3.mlp GPT2MLP\n",
      "transformer.h.3.mlp.c_fc Conv1D\n",
      "transformer.h.3.mlp.c_proj Conv1D\n",
      "transformer.h.3.mlp.act NewGELUActivation\n",
      "transformer.h.3.mlp.dropout Dropout\n",
      "transformer.h.4 GPT2Block\n",
      "transformer.h.4.ln_1 LayerNorm\n",
      "transformer.h.4.attn GPT2Attention\n",
      "transformer.h.4.attn.c_attn Conv1D\n",
      "transformer.h.4.attn.c_proj Conv1D\n",
      "transformer.h.4.attn.attn_dropout Dropout\n",
      "transformer.h.4.attn.resid_dropout Dropout\n",
      "transformer.h.4.ln_2 LayerNorm\n",
      "transformer.h.4.mlp GPT2MLP\n",
      "transformer.h.4.mlp.c_fc Conv1D\n",
      "transformer.h.4.mlp.c_proj Conv1D\n",
      "transformer.h.4.mlp.act NewGELUActivation\n",
      "transformer.h.4.mlp.dropout Dropout\n",
      "transformer.h.5 GPT2Block\n",
      "transformer.h.5.ln_1 LayerNorm\n",
      "transformer.h.5.attn GPT2Attention\n",
      "transformer.h.5.attn.c_attn Conv1D\n",
      "transformer.h.5.attn.c_proj Conv1D\n",
      "transformer.h.5.attn.attn_dropout Dropout\n",
      "transformer.h.5.attn.resid_dropout Dropout\n",
      "transformer.h.5.ln_2 LayerNorm\n",
      "transformer.h.5.mlp GPT2MLP\n",
      "transformer.h.5.mlp.c_fc Conv1D\n",
      "transformer.h.5.mlp.c_proj Conv1D\n",
      "transformer.h.5.mlp.act NewGELUActivation\n",
      "transformer.h.5.mlp.dropout Dropout\n",
      "transformer.h.6 GPT2Block\n",
      "transformer.h.6.ln_1 LayerNorm\n",
      "transformer.h.6.attn GPT2Attention\n",
      "transformer.h.6.attn.c_attn Conv1D\n",
      "transformer.h.6.attn.c_proj Conv1D\n",
      "transformer.h.6.attn.attn_dropout Dropout\n",
      "transformer.h.6.attn.resid_dropout Dropout\n",
      "transformer.h.6.ln_2 LayerNorm\n",
      "transformer.h.6.mlp GPT2MLP\n",
      "transformer.h.6.mlp.c_fc Conv1D\n",
      "transformer.h.6.mlp.c_proj Conv1D\n",
      "transformer.h.6.mlp.act NewGELUActivation\n",
      "transformer.h.6.mlp.dropout Dropout\n",
      "transformer.h.7 GPT2Block\n",
      "transformer.h.7.ln_1 LayerNorm\n",
      "transformer.h.7.attn GPT2Attention\n",
      "transformer.h.7.attn.c_attn Conv1D\n",
      "transformer.h.7.attn.c_proj Conv1D\n",
      "transformer.h.7.attn.attn_dropout Dropout\n",
      "transformer.h.7.attn.resid_dropout Dropout\n",
      "transformer.h.7.ln_2 LayerNorm\n",
      "transformer.h.7.mlp GPT2MLP\n",
      "transformer.h.7.mlp.c_fc Conv1D\n",
      "transformer.h.7.mlp.c_proj Conv1D\n",
      "transformer.h.7.mlp.act NewGELUActivation\n",
      "transformer.h.7.mlp.dropout Dropout\n",
      "transformer.h.8 GPT2Block\n",
      "transformer.h.8.ln_1 LayerNorm\n",
      "transformer.h.8.attn GPT2Attention\n",
      "transformer.h.8.attn.c_attn Conv1D\n",
      "transformer.h.8.attn.c_proj Conv1D\n",
      "transformer.h.8.attn.attn_dropout Dropout\n",
      "transformer.h.8.attn.resid_dropout Dropout\n",
      "transformer.h.8.ln_2 LayerNorm\n",
      "transformer.h.8.mlp GPT2MLP\n",
      "transformer.h.8.mlp.c_fc Conv1D\n",
      "transformer.h.8.mlp.c_proj Conv1D\n",
      "transformer.h.8.mlp.act NewGELUActivation\n",
      "transformer.h.8.mlp.dropout Dropout\n",
      "transformer.h.9 GPT2Block\n",
      "transformer.h.9.ln_1 LayerNorm\n",
      "transformer.h.9.attn GPT2Attention\n",
      "transformer.h.9.attn.c_attn Conv1D\n",
      "transformer.h.9.attn.c_proj Conv1D\n",
      "transformer.h.9.attn.attn_dropout Dropout\n",
      "transformer.h.9.attn.resid_dropout Dropout\n",
      "transformer.h.9.ln_2 LayerNorm\n",
      "transformer.h.9.mlp GPT2MLP\n",
      "transformer.h.9.mlp.c_fc Conv1D\n",
      "transformer.h.9.mlp.c_proj Conv1D\n",
      "transformer.h.9.mlp.act NewGELUActivation\n",
      "transformer.h.9.mlp.dropout Dropout\n",
      "transformer.h.10 GPT2Block\n",
      "transformer.h.10.ln_1 LayerNorm\n",
      "transformer.h.10.attn GPT2Attention\n",
      "transformer.h.10.attn.c_attn Conv1D\n",
      "transformer.h.10.attn.c_proj Conv1D\n",
      "transformer.h.10.attn.attn_dropout Dropout\n",
      "transformer.h.10.attn.resid_dropout Dropout\n",
      "transformer.h.10.ln_2 LayerNorm\n",
      "transformer.h.10.mlp GPT2MLP\n",
      "transformer.h.10.mlp.c_fc Conv1D\n",
      "transformer.h.10.mlp.c_proj Conv1D\n",
      "transformer.h.10.mlp.act NewGELUActivation\n",
      "transformer.h.10.mlp.dropout Dropout\n",
      "transformer.h.11 GPT2Block\n",
      "transformer.h.11.ln_1 LayerNorm\n",
      "transformer.h.11.attn GPT2Attention\n",
      "transformer.h.11.attn.c_attn Conv1D\n",
      "transformer.h.11.attn.c_proj Conv1D\n",
      "transformer.h.11.attn.attn_dropout Dropout\n",
      "transformer.h.11.attn.resid_dropout Dropout\n",
      "transformer.h.11.ln_2 LayerNorm\n",
      "transformer.h.11.mlp GPT2MLP\n",
      "transformer.h.11.mlp.c_fc Conv1D\n",
      "transformer.h.11.mlp.c_proj Conv1D\n",
      "transformer.h.11.mlp.act NewGELUActivation\n",
      "transformer.h.11.mlp.dropout Dropout\n",
      "transformer.ln_f LayerNorm\n",
      "lm_head Linear\n",
      "Warning: LoRA branch 'bw999' not found in transformer.h.0.attn.c_attn, using None\n",
      "Sampled:  going to be a lot more interesting than it was before.\n",
      "I'm not sure if I'm goin...\n",
      "✅ Step 2 completed: 2 wrappers, 12 params\n"
     ]
    }
   ],
   "source": [
    "# Step 2: LoRA migration test\n",
    "from src.lora import attach_lora_to_quant, activate_lora_by_config, activate_lora_by_bits\n",
    "\n",
    "# Set same random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# LoRA spec with 6-bit support (same as test1)\n",
    "lora_spec = {\n",
    "    \"transformer.h.0.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8), \"bw6\": (6,12)},\n",
    "    \"transformer.h.1.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8), \"bw6\": (6,12)},\n",
    "}\n",
    "\n",
    "# Fresh model + quantization + LoRA\n",
    "lora_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "wrappers = attach_lora_to_quant(lora_model, lora_spec, cfg)\n",
    "\n",
    "# Test 6-bit activation and fallback\n",
    "test_6bit_cfg = {\"per_layer_bits\": {\"transformer.h.0.attn.c_attn\": 6}, \"default_w_bits\": 8}\n",
    "activate_lora_by_config(wrappers, test_6bit_cfg)\n",
    "\n",
    "# Test branch switching and parameter collection\n",
    "test_wrapper = list(wrappers.values())[0]\n",
    "test_wrapper.set_active(\"bw4\")\n",
    "test_wrapper.set_active(\"bw999\")  # trigger fallback warning\n",
    "\n",
    "lora_params = [p for w in wrappers.values() for p in w.bank.parameters() if p.requires_grad]\n",
    "\n",
    "# Set consistent activation state for generation test\n",
    "for wrapper in wrappers.values():\n",
    "    wrapper.set_active(\"bw8\")  # ensure same activation state\n",
    "\n",
    "# Deterministic generation test (minimal)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "lora_model.eval()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inp = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = lora_model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "res = tokenizer.decode(out[0, inp[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "print(\"Sampled:\", res[:80] + \"...\")\n",
    "\n",
    "print(f\"✅ Step 2 completed: {len(wrappers)} wrappers, {len(lora_params)} params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58596257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000 | Loss: 3.307 | Config: 4-bit: 100%|██████████| 1000/1000 [01:35<00:00, 10.46it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "After training: Hello, world is  going to be a lot more interesting than it was before.\n",
      "I'm not ...\n",
      "✅ Step 3 completed: switchable training with 2 configs\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Switchable precision training test\n",
    "from src.training import create_squad_dataloader, SwitchableTrainer\n",
    "from src.quantization import load_config\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepare multiple configs for switchable training\n",
    "config_A = load_config(\"../configs/config.yaml\")\n",
    "config_B = load_config(\"../configs/config_4bit.yaml\")\n",
    "precision_configs = [config_A, config_B]\n",
    "\n",
    "# Create dataloader and trainer\n",
    "train_dataloader = create_squad_dataloader(tokenizer, batch_size=4, subset_size=100)\n",
    "trainer = SwitchableTrainer(lora_model, wrappers, precision_configs, lr=1e-4)\n",
    "\n",
    "# Run training (1000 iterations)\n",
    "trainer.train(train_dataloader, iterations=1000)\n",
    "\n",
    "# Test generation after training (CUDA-only)\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"CUDA is required for generation but not found.\"\n",
    "\n",
    "lora_model.eval().to(\"cuda\")\n",
    "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model.generate(**inputs, max_length=50)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(\"After training:\", result[:80] + \"...\")\n",
    "\n",
    "print(f\"✅ Step 3 completed: switchable training with {len(precision_configs)} configs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluation test using src modules\n",
    "from src.evaluation import analyze_all_configs, save_results\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load validation dataset\n",
    "val_ds = load_dataset(\"squad\", split=\"validation\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Analyze configs using migrated functions\n",
    "df = analyze_all_configs(lora_model, tokenizer, val_ds, device, n=50)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "save_results(df.to_dict('records'), \"step4_results.json\")\n",
    "\n",
    "print(f\"✅ Step 4 completed: evaluated {len(df)} configs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
