{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is  going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter \n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# test generate\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=68)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   995,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "for name, mod in model.named_modules():\n",
        "    print(name, mod.__class__.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "dict_keys(['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj'])\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"../configs/config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "print(cfg[\"default_w_bits\"])       \n",
        "print(cfg[\"per_layer_bits\"].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import torch.nn as nn\n",
        "\n",
        "def want_quant(name, mod, cfg):\n",
        "    # skip embedding / norm / lm_head\n",
        "    # only Linear or Conv1D\n",
        "    if name == \"lm_head\": \n",
        "        return False\n",
        "    if isinstance(mod, nn.Linear) or mod.__class__.__name__ == \"Conv1D\":\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768, 2304)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.named_modules())[10][1].weight.shape\n",
        "a,b = list(model.named_modules())[9][1].weight.shape\n",
        "a,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantLinear(nn.Module):\n",
        "    r\"\"\"Quantized version of nn.Linear.\n",
        "\n",
        "    This layer works just like nn.Linear, but the weights are stored\n",
        "    in int8 format to save memory and improve efficiency.\n",
        "\n",
        "    Instead of a single global scale, each output channel has its own\n",
        "    scale factor. This makes the quantization more accurate because\n",
        "    different output channels can have very different weight ranges.\n",
        "\n",
        "    Input shape:  (*, in_features)\n",
        "    Output shape: (*, out_features)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        bias: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.register_buffer(\"qweight\",\n",
        "            torch.empty(out_features, in_features, dtype=torch.int8, device=device))\n",
        "        self.register_buffer(\"w_scale\",\n",
        "            torch.ones(out_features, dtype=torch.float32, device=device))\n",
        "        self.register_buffer(\"w_zp\",\n",
        "            torch.zeros(out_features, dtype=torch.int32, device=device))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if torch.any(self.w_zp != 0):\n",
        "            # non-zero： (q - zp) * scale\n",
        "            W = (self.qweight.int() - self.w_zp.view(-1, 1)).float() * self.w_scale.view(-1, 1)\n",
        "        else:\n",
        "            # zero w zp： q * scale\n",
        "            W = self.qweight.float() * self.w_scale.view(-1, 1)\n",
        "        return F.linear(input, W, self.bias)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the extra representation of the quant module.\n",
        "        \"\"\"\n",
        "        return (f\"in_features={self.in_features}, out_features={self.out_features}, \"\n",
        "                f\"bias={self.bias is not None}, dtype=int8, per_channel=True\")\n",
        "                \n",
        "    @staticmethod\n",
        "    def get_bits_for_layer(name: str, cfg: dict) -> int:\n",
        "        return cfg[\"per_layer_bits\"][name]\n",
        "\n",
        "    def quantize_from_float(self, weight: torch.Tensor, bits: int = 8):\n",
        "        # support 2-8 bits\n",
        "        qmin, qmax = -(2**(bits-1)), 2**(bits-1) - 1 \n",
        "        w_max_abs = weight.abs().max(dim=1, keepdim=True)[0]\n",
        "        w_max_abs = torch.clamp(w_max_abs, min=1e-8)\n",
        "        scale = w_max_abs / qmax\n",
        "        qweight = torch.clamp(torch.round(weight / scale), qmin, qmax).to(torch.int8)\n",
        "        zero_point = torch.zeros(weight.size(0), dtype=torch.int32, device=weight.device)\n",
        "        self.qweight.copy_(qweight)\n",
        "        self.w_scale.copy_(scale.squeeze())\n",
        "        self.w_zp.copy_(zero_point)\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, base: nn.Linear, name: str, cfg: dict):\n",
        "        bits = cls.get_bits_for_layer(name, cfg)\n",
        "        q = cls(base.in_features, base.out_features,\n",
        "                bias=(base.bias is not None),\n",
        "                device=base.weight.device, dtype=base.weight.dtype)\n",
        "        with torch.no_grad():\n",
        "            q.quantize_from_float(base.weight, bits=bits)\n",
        "            if base.bias is not None:\n",
        "                q.bias.copy_(base.bias)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(QuantLinear.get_bits_for_layer(\"transformer.h.0.attn.c_attn\", cfg))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "def replace_with_quant(model, cfg):\n",
        "    name_to_module = dict(model.named_modules())\n",
        "    for name, mod in list(name_to_module.items()):\n",
        "        print(name, mod.__class__.__name__)\n",
        "        if not want_quant(name, mod, cfg):\n",
        "            continue\n",
        "\n",
        "        # parent module location\n",
        "        if '.' in name:\n",
        "            parent_name, child_name = name.rsplit('.', 1)\n",
        "            parent = name_to_module[parent_name]\n",
        "        else:\n",
        "            parent, child_name = model, name\n",
        "\n",
        "        # convert Conv1D to Linear\n",
        "        if mod.__class__.__name__ == \"Conv1D\":\n",
        "                in_f, out_f = mod.weight.shape          # Conv1D weight is (out, in)\n",
        "                base = nn.Linear(in_f, out_f, bias=(mod.bias is not None))\n",
        "                base.to(mod.weight.device, dtype=mod.weight.dtype)\n",
        "                with torch.no_grad():\n",
        "                    base.weight.copy_(mod.weight.T)       # copy \n",
        "                    if mod.bias is not None:\n",
        "                        base.bias.copy_(mod.bias)\n",
        "        else:\n",
        "            base = mod\n",
        "\n",
        "        # construct quantization wrapper\n",
        "        qcfg = cfg.copy()\n",
        "        qmod = QuantLinear.from_linear(base, name,cfg=qcfg)\n",
        "\n",
        "        # replace the original layer with setattr\n",
        "        setattr(parent, child_name, qmod)\n",
        "\n",
        "replace_with_quant(model, cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is  going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=68)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, in_f, out_f, r=4, alpha=None):\n",
        "        super().__init__()\n",
        "        self.scale = (alpha or r) / r\n",
        "        self.A = nn.Parameter(torch.randn(r, in_f) * 0.01)\n",
        "        self.B = nn.Parameter(torch.zeros(out_f, r))\n",
        "    def forward(self, x):\n",
        "        return (x @ self.A.t()) @ self.B.t() * self.scale\n",
        "\n",
        "class LoRAWrapped(nn.Module):\n",
        "    def __init__(self, base, branches, layer_name=None):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # keep device and precision\n",
        "        dev = next(self.base.parameters()).device\n",
        "        dtype = next(self.base.parameters()).dtype\n",
        "\n",
        "        in_f = self.base.in_features\n",
        "        out_f = self.base.out_features\n",
        "\n",
        "        self.bank = nn.ModuleDict({\n",
        "            k: LoRA(in_f, out_f, r, a).to(device=dev, dtype=dtype)\n",
        "            for k, (r, a) in branches.items()\n",
        "        })\n",
        "        self.active = None            # only one branch\n",
        "        self.layer_name = layer_name  # for logging / routing (optional)\n",
        "\n",
        "    def set_active(self, name_or_none):\n",
        "        self.active = name_or_none   # name_or_none: 'bw4' / 'bw8' / None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.base(x)\n",
        "        if self.active in self.bank:\n",
        "            y = y + self.bank[self.active](x)\n",
        "        return y\n",
        "\n",
        "def attach_lora_to_quant(model, name2branches, quant_cfg):\n",
        "    \"\"\"\n",
        "    Lora and quant at the same time\n",
        "    \"\"\"\n",
        "    # quant\n",
        "    replace_with_quant(model, quant_cfg)\n",
        "    \n",
        "    # add LoRA\n",
        "    wrappers = {}\n",
        "    for name, mod in list(model.named_modules()):\n",
        "        if name in name2branches and hasattr(mod, 'in_features'):\n",
        "            parent = model.get_submodule(name.rsplit('.',1)[0]) if '.' in name else model\n",
        "            attr = name.split('.')[-1]\n",
        "            w = LoRAWrapped(mod, name2branches[name], layer_name=name)\n",
        "            setattr(parent, attr, w)\n",
        "            wrappers[name] = w\n",
        "    return wrappers\n",
        "\n",
        "# activate by bit config (call this before inference)\n",
        "def activate_lora_by_bits(wrappers, bit_cfg, default_bits=None):\n",
        "    m = {4: \"bw4\", 8: \"bw8\"}  # 4/8\n",
        "    for n, w in wrappers.items():\n",
        "        bw = bit_cfg.get(n, default_bits)\n",
        "        w.set_active(m[bw])  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn QuantLinear\n",
            "transformer.h.0.attn.c_proj QuantLinear\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc QuantLinear\n",
            "transformer.h.0.mlp.c_proj QuantLinear\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn QuantLinear\n",
            "transformer.h.1.attn.c_proj QuantLinear\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc QuantLinear\n",
            "transformer.h.1.mlp.c_proj QuantLinear\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn QuantLinear\n",
            "transformer.h.2.attn.c_proj QuantLinear\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc QuantLinear\n",
            "transformer.h.2.mlp.c_proj QuantLinear\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn QuantLinear\n",
            "transformer.h.3.attn.c_proj QuantLinear\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc QuantLinear\n",
            "transformer.h.3.mlp.c_proj QuantLinear\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn QuantLinear\n",
            "transformer.h.4.attn.c_proj QuantLinear\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc QuantLinear\n",
            "transformer.h.4.mlp.c_proj QuantLinear\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn QuantLinear\n",
            "transformer.h.5.attn.c_proj QuantLinear\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc QuantLinear\n",
            "transformer.h.5.mlp.c_proj QuantLinear\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn QuantLinear\n",
            "transformer.h.6.attn.c_proj QuantLinear\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc QuantLinear\n",
            "transformer.h.6.mlp.c_proj QuantLinear\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn QuantLinear\n",
            "transformer.h.7.attn.c_proj QuantLinear\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc QuantLinear\n",
            "transformer.h.7.mlp.c_proj QuantLinear\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn QuantLinear\n",
            "transformer.h.8.attn.c_proj QuantLinear\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc QuantLinear\n",
            "transformer.h.8.mlp.c_proj QuantLinear\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn QuantLinear\n",
            "transformer.h.9.attn.c_proj QuantLinear\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc QuantLinear\n",
            "transformer.h.9.mlp.c_proj QuantLinear\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn QuantLinear\n",
            "transformer.h.10.attn.c_proj QuantLinear\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc QuantLinear\n",
            "transformer.h.10.mlp.c_proj QuantLinear\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn QuantLinear\n",
            "transformer.h.11.attn.c_proj QuantLinear\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc QuantLinear\n",
            "transformer.h.11.mlp.c_proj QuantLinear\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "# define the quantization bit width and LoRA branches for each layer\n",
        "lora_spec = {\n",
        "    \"transformer.h.0.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    \"transformer.h.1.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    # ... more layers\n",
        "}\n",
        "\n",
        "# one-step: quant + LoRA\n",
        "wrappers = attach_lora_to_quant(model, lora_spec, cfg)\n",
        "\n",
        "# \n",
        "for name, wrapper in wrappers.items():\n",
        "    bits = cfg[\"per_layer_bits\"].get(name, cfg[\"default_w_bits\"])\n",
        "    if bits <= 4:\n",
        "        wrapper.set_active(\"bw4\")\n",
        "    else:   \n",
        "        wrapper.set_active(\"bw8\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
