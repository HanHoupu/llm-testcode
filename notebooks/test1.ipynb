{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is Â going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter \n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# test generate\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=68)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   995,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "for name, mod in model.named_modules():\n",
        "    print(name, mod.__class__.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "dict_keys(['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj'])\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"../configs/config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "print(cfg[\"default_w_bits\"])       \n",
        "print(cfg[\"per_layer_bits\"].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import torch.nn as nn\n",
        "\n",
        "def want_quant(name, mod, cfg):\n",
        "    # skip embedding / norm / lm_head\n",
        "    # only Linear or Conv1D\n",
        "    if name == \"lm_head\": \n",
        "        return False\n",
        "    if isinstance(mod, nn.Linear) or mod.__class__.__name__ == \"Conv1D\":\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768, 2304)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.named_modules())[10][1].weight.shape\n",
        "a,b = list(model.named_modules())[9][1].weight.shape\n",
        "a,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantLinear(nn.Module):\n",
        "    r\"\"\"Quantized version of nn.Linear.\n",
        "\n",
        "    This layer works just like nn.Linear, but the weights are stored\n",
        "    in int8 format to save memory and improve efficiency.\n",
        "\n",
        "    Instead of a single global scale, each output channel has its own\n",
        "    scale factor. This makes the quantization more accurate because\n",
        "    different output channels can have very different weight ranges.\n",
        "\n",
        "    Input shape:  (*, in_features)\n",
        "    Output shape: (*, out_features)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        bias: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.register_buffer(\"qweight\",\n",
        "            torch.empty(out_features, in_features, dtype=torch.int8, device=device))\n",
        "        self.register_buffer(\"w_scale\",\n",
        "            torch.ones(out_features, dtype=torch.float32, device=device))\n",
        "        self.register_buffer(\"w_zp\",\n",
        "            torch.zeros(out_features, dtype=torch.int32, device=device))\n",
        "        self.register_buffer(\"fp32_weight\", None)  # orignal\n",
        "        self.current_bits = None  # current \n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "    def store_fp32_weight(self, weight: torch.Tensor):\n",
        "        self.fp32_weight = weight.clone()\n",
        "\n",
        "    def requantize_to_bits(self, bits: int):\n",
        "        if self.current_bits == bits:\n",
        "            return \n",
        "        \n",
        "        self.quantize_from_float(self.fp32_weight, bits=bits)\n",
        "        self.current_bits = bits\n",
        "\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if torch.any(self.w_zp != 0):\n",
        "            # non-zeroï¼ (q - zp) * scale\n",
        "            W = (self.qweight.int() - self.w_zp.view(-1, 1)).float() * self.w_scale.view(-1, 1)\n",
        "        else:\n",
        "            # zero w zpï¼ q * scale\n",
        "            W = self.qweight.float() * self.w_scale.view(-1, 1)\n",
        "        return F.linear(input, W, self.bias)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the extra representation of the quant module.\n",
        "        \"\"\"\n",
        "        return (f\"in_features={self.in_features}, out_features={self.out_features}, \"\n",
        "                f\"bias={self.bias is not None}, dtype=int8, per_channel=True\")\n",
        "                \n",
        "    @staticmethod\n",
        "    def get_bits_for_layer(name: str, cfg: dict) -> int:\n",
        "        return cfg[\"per_layer_bits\"][name]\n",
        "\n",
        "    def quantize_from_float(self, weight: torch.Tensor, bits: int = 8):\n",
        "        # support 2-8 bits\n",
        "        qmin, qmax = -(2**(bits-1)), 2**(bits-1) - 1 \n",
        "        w_max_abs = weight.abs().max(dim=1, keepdim=True)[0]\n",
        "        w_max_abs = torch.clamp(w_max_abs, min=1e-8)\n",
        "        scale = w_max_abs / qmax\n",
        "        qweight = torch.clamp(torch.round(weight / scale), qmin, qmax).to(torch.int8)\n",
        "        zero_point = torch.zeros(weight.size(0), dtype=torch.int32, device=weight.device)\n",
        "        self.qweight.copy_(qweight)\n",
        "        self.w_scale.copy_(scale.squeeze())\n",
        "        self.w_zp.copy_(zero_point)\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, base: nn.Linear, name: str, cfg: dict):\n",
        "        bits = cls.get_bits_for_layer(name, cfg)\n",
        "        q = cls(base.in_features, base.out_features,\n",
        "                bias=(base.bias is not None),\n",
        "                device=base.weight.device, dtype=base.weight.dtype)\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            q.store_fp32_weight(base.weight)\n",
        "\n",
        "            bits = cls.get_bits_for_layer(name, cfg)\n",
        "            q.quantize_from_float(base.weight, bits=bits)\n",
        "            q.current_bits = bits\n",
        "            \n",
        "            if base.bias is not None:\n",
        "                q.bias.copy_(base.bias)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(QuantLinear.get_bits_for_layer(\"transformer.h.0.attn.c_attn\", cfg))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def requantize_model_to_config(model, cfg):\n",
        "        \n",
        "    default_bits = cfg.get('default_w_bits', 8)\n",
        "    per_layer_bits = cfg.get('per_layer_bits', {})\n",
        "    \n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, QuantLinear):\n",
        "            target_bits = per_layer_bits.get(name, default_bits)\n",
        "            module.requantize_to_bits(target_bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "def replace_with_quant(model, cfg):\n",
        "    name_to_module = dict(model.named_modules())\n",
        "    for name, mod in list(name_to_module.items()):\n",
        "        print(name, mod.__class__.__name__)\n",
        "        if not want_quant(name, mod, cfg):\n",
        "            continue\n",
        "\n",
        "        # parent module location\n",
        "        if '.' in name:\n",
        "            parent_name, child_name = name.rsplit('.', 1)\n",
        "            parent = name_to_module[parent_name]\n",
        "        else:\n",
        "            parent, child_name = model, name\n",
        "\n",
        "        # convert Conv1D to Linear\n",
        "        if mod.__class__.__name__ == \"Conv1D\":\n",
        "                in_f, out_f = mod.weight.shape          # Conv1D weight is (out, in)\n",
        "                base = nn.Linear(in_f, out_f, bias=(mod.bias is not None))\n",
        "                base.to(mod.weight.device, dtype=mod.weight.dtype)\n",
        "                with torch.no_grad():\n",
        "                    base.weight.copy_(mod.weight.T)       # copy \n",
        "                    if mod.bias is not None:\n",
        "                        base.bias.copy_(mod.bias)\n",
        "        else:\n",
        "            base = mod\n",
        "\n",
        "        # construct quantization wrapper\n",
        "        qcfg = cfg.copy()\n",
        "        qmod = QuantLinear.from_linear(base, name,cfg=qcfg)\n",
        "\n",
        "        # replace the original layer with setattr\n",
        "        setattr(parent, child_name, qmod)\n",
        "\n",
        "replace_with_quant(model, cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n",
            "Result: Hello, world is Â in theÂ world.Â \n",
            "I've been in theÂ aÂ smallÂ smallÂ smallÂ smallÂ small\n"
          ]
        }
      ],
      "source": [
        "# Step 1 test - fresh model with original methods\n",
        "test_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "replace_with_quant(test_model, cfg)\n",
        "\n",
        "# Test 2bit/6bit requantization  \n",
        "with open(\"../configs/test_2bit_6bit.yaml\", \"r\") as f:\n",
        "    test_cfg = yaml.safe_load(f)\n",
        "requantize_model_to_config(test_model, test_cfg)\n",
        "\n",
        "# Generation test\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = test_model.generate(**inputs, max_length=30)\n",
        "print(\"Result:\", tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is Â going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=68)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, in_f, out_f, r=4, alpha=None):\n",
        "        super().__init__()\n",
        "        self.scale = (alpha or r) / r\n",
        "        self.A = nn.Parameter(torch.randn(r, in_f) * 0.01)\n",
        "        self.B = nn.Parameter(torch.zeros(out_f, r))\n",
        "    def forward(self, x):\n",
        "        return (x @ self.A.t()) @ self.B.t() * self.scale\n",
        "\n",
        "class LoRAWrapped(nn.Module):\n",
        "    def __init__(self, base, branches, layer_name=None):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # keep device and precision\n",
        "        dev = next(self.base.parameters()).device\n",
        "        dtype = next(self.base.parameters()).dtype\n",
        "\n",
        "        in_f = self.base.in_features\n",
        "        out_f = self.base.out_features\n",
        "\n",
        "        self.bank = nn.ModuleDict({\n",
        "            k: LoRA(in_f, out_f, r, a).to(device=dev, dtype=dtype)\n",
        "            for k, (r, a) in branches.items()\n",
        "        })\n",
        "        self.active = None            # only one branch\n",
        "        self.layer_name = layer_name  # for logging / routing (optional)\n",
        "\n",
        "    def set_active(self, name_or_none):\n",
        "        self.active = name_or_none   # name_or_none: 'bw4' / 'bw8' / None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.base(x)\n",
        "        if self.active in self.bank:\n",
        "            y = y + self.bank[self.active](x)\n",
        "        return y\n",
        "\n",
        "def attach_lora_to_quant(model, name2branches, quant_cfg):\n",
        "    \"\"\"\n",
        "    Lora and quant at the same time\n",
        "    \"\"\"\n",
        "    # quant\n",
        "    replace_with_quant(model, quant_cfg)\n",
        "    \n",
        "    # add LoRA\n",
        "    wrappers = {}\n",
        "    for name, mod in list(model.named_modules()):\n",
        "        if name in name2branches and hasattr(mod, 'in_features'):\n",
        "            parent = model.get_submodule(name.rsplit('.',1)[0]) if '.' in name else model\n",
        "            attr = name.split('.')[-1]\n",
        "            w = LoRAWrapped(mod, name2branches[name], layer_name=name)\n",
        "            setattr(parent, attr, w)\n",
        "            wrappers[name] = w\n",
        "    return wrappers\n",
        "\n",
        "# activate by bit config (call this before inference)\n",
        "def activate_lora_by_bits(wrappers, bit_cfg, default_bits=None):\n",
        "    m = {4: \"bw4\", 8: \"bw8\"}  # 4/8\n",
        "    for n, w in wrappers.items():\n",
        "        bw = bit_cfg.get(n, default_bits)\n",
        "        w.set_active(m[bw])  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn QuantLinear\n",
            "transformer.h.0.attn.c_proj QuantLinear\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc QuantLinear\n",
            "transformer.h.0.mlp.c_proj QuantLinear\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn QuantLinear\n",
            "transformer.h.1.attn.c_proj QuantLinear\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc QuantLinear\n",
            "transformer.h.1.mlp.c_proj QuantLinear\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn QuantLinear\n",
            "transformer.h.2.attn.c_proj QuantLinear\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc QuantLinear\n",
            "transformer.h.2.mlp.c_proj QuantLinear\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn QuantLinear\n",
            "transformer.h.3.attn.c_proj QuantLinear\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc QuantLinear\n",
            "transformer.h.3.mlp.c_proj QuantLinear\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn QuantLinear\n",
            "transformer.h.4.attn.c_proj QuantLinear\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc QuantLinear\n",
            "transformer.h.4.mlp.c_proj QuantLinear\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn QuantLinear\n",
            "transformer.h.5.attn.c_proj QuantLinear\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc QuantLinear\n",
            "transformer.h.5.mlp.c_proj QuantLinear\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn QuantLinear\n",
            "transformer.h.6.attn.c_proj QuantLinear\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc QuantLinear\n",
            "transformer.h.6.mlp.c_proj QuantLinear\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn QuantLinear\n",
            "transformer.h.7.attn.c_proj QuantLinear\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc QuantLinear\n",
            "transformer.h.7.mlp.c_proj QuantLinear\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn QuantLinear\n",
            "transformer.h.8.attn.c_proj QuantLinear\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc QuantLinear\n",
            "transformer.h.8.mlp.c_proj QuantLinear\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn QuantLinear\n",
            "transformer.h.9.attn.c_proj QuantLinear\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc QuantLinear\n",
            "transformer.h.9.mlp.c_proj QuantLinear\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn QuantLinear\n",
            "transformer.h.10.attn.c_proj QuantLinear\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc QuantLinear\n",
            "transformer.h.10.mlp.c_proj QuantLinear\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn QuantLinear\n",
            "transformer.h.11.attn.c_proj QuantLinear\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc QuantLinear\n",
            "transformer.h.11.mlp.c_proj QuantLinear\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "# define the quantization bit width and LoRA branches for each layer\n",
        "lora_spec = {\n",
        "    \"transformer.h.0.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    \"transformer.h.1.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    # ... more layers\n",
        "}\n",
        "\n",
        "# one-step: quant + LoRA\n",
        "wrappers = attach_lora_to_quant(model, lora_spec, cfg)\n",
        "\n",
        "# \n",
        "for name, wrapper in wrappers.items():\n",
        "    bits = cfg[\"per_layer_bits\"].get(name, cfg[\"default_w_bits\"])\n",
        "    if bits <= 4:\n",
        "        wrapper.set_active(\"bw4\")\n",
        "    else:   \n",
        "        wrapper.set_active(\"bw8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "squad_dataset = load_dataset(\"squad\", split=\"train\")\n",
        "\n",
        "# Format dataset to prompt\n",
        "def format_squad_prompt(sample):\n",
        "    return f\"question: {sample['question']} context: {sample['context']} answer: {sample['answers']['text'][0]}\"\n",
        "\n",
        "# Create a small subset\n",
        "subset_indices = random.sample(range(len(squad_dataset)), 1000)\n",
        "squad_subset = squad_dataset.select(subset_indices)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    prompts = [format_squad_prompt(s) for s in batch]\n",
        "    return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "train_dataloader = DataLoader(squad_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load two config files directly into a list\n",
        "with open(\"../configs/config.yaml\", 'r') as f: config_A = yaml.safe_load(f)\n",
        "with open(\"../configs/config_4bit.yaml\", 'r') as f: config_B = yaml.safe_load(f)\n",
        "\n",
        "precision_configs = [config_A, config_B]\n",
        "\n",
        "# Collect all LoRA module parameters\n",
        "lora_params = [\n",
        "    p for w in wrappers.values() for p in w.bank.parameters() if p.requires_grad\n",
        "]\n",
        "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "Iteration 1000 | Loss: 3.854 | Config: 8-bit: 100%|ââââââââââ| 1000/1000 [00:43<00:00, 22.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.train()  # Set to training mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Use tqdm to create a progress bar\n",
        "progress_bar = tqdm(range(1000))\n",
        "\n",
        "# Get data from dataloader\n",
        "data_iter = iter(train_dataloader)\n",
        "\n",
        "for i in progress_bar:\n",
        "    # If data is used up, create a new iterator\n",
        "    try:\n",
        "        batch = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(train_dataloader)\n",
        "        batch = next(data_iter)\n",
        "\n",
        "    # Randomly select a precision configuration\n",
        "    chosen_config = random.choice(precision_configs)\n",
        "\n",
        "    # Activate the corresponding LoRA branch based on the selected configuration\n",
        "    per_layer_config = chosen_config.get('per_layer_bits', {})\n",
        "    default_bits = chosen_config.get('default_w_bits')\n",
        "    activate_lora_by_bits(wrappers, per_layer_config, default_bits)\n",
        "\n",
        "    #training process\n",
        "    inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Update the progress bar display\n",
        "    progress_bar.set_description(f\"Iteration {i+1} | Loss: {loss.item():.3f} | Config: {chosen_config['default_w_bits']}-bit\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[A] default8 => France is a country in Western Europe. Its capital and largest city is Paris. Its capital and largest city is Paris. Its capital and largest city is Paris.\n",
            "[B] default4 => France is a country in Western Europe. Its capital and largest city is Paris. Its capital and largest city is Paris. Its capital and largest city is Paris.\n"
          ]
        }
      ],
      "source": [
        "# --- Quick smoke test for 2 configs ---\n",
        "model.eval()\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "q = \"What is the capital of France?\"\n",
        "c = \"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
        "prompt = f\"question: {q} context: {c} answer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_with(cfg):\n",
        "    activate_lora_by_bits(wrappers, cfg.get('per_layer_bits', {}), cfg.get('default_w_bits'))\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=32, do_sample=False,  # æ´ç¨³å®\n",
        "        pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    gen = out[0, inputs['input_ids'].shape[1]:]      # åªåæ°çæé¨å\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"[A]\", cfg_A_name := config_A.get(\"name\", f\"default{config_A.get('default_w_bits')}\"), \"=>\", gen_with(config_A))\n",
        "print(\"[B]\", cfg_B_name := config_B.get(\"name\", f\"default{config_B.get('default_w_bits')}\"), \"=>\", gen_with(config_B))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è¯ä¼°å°ä½¿ç¨è®¾å¤: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval 8-bit: 100%|ââââââââââ| 250/250 [00:09<00:00, 26.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config_default_8] bits=8 | loss=3.468 | ppl=32.06 | tokens/s=35365 | peak=1620MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval 4-bit: 100%|ââââââââââ| 250/250 [00:09<00:00, 27.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config_default_4] bits=4 | loss=3.719 | ppl=41.22 | tokens/s=35353 | peak=1620MB\n",
            "\n",
            "== Leaderboard (by PPL) ==\n",
            "config_default_8 | ppl=32.06 | loss=3.468 | mem=1620MB | tps=35365\n",
            "config_default_4 | ppl=41.22 | loss=3.719 | mem=1620MB | tps=35353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# cell 16 (ä¿®æ¹å)\n",
        "import math, time, torch, random, gc\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. å¨ Cell çæå¼å§å°±å®ä¹å¥½å¨å±è®¾å¤ ---\n",
        "# è¿æ ·åç»­æææä½é½ä¼é»è®¤ä½¿ç¨è¿ä¸ªè®¾å¤\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"è¯ä¼°å°ä½¿ç¨è®¾å¤: {device}\")\n",
        "\n",
        "model.to(device) # ç¡®ä¿æ¨¡åå¨æ­£ç¡®çè®¾å¤ä¸\n",
        "model.eval()\n",
        "\n",
        "# 2) åå¤éªè¯é (è¿é¨åä¸å)\n",
        "val_ds = load_dataset(\"squad\", split=\"validation\")\n",
        "def fmt(s): return f\"question: {s['question']} context: {s['context']} answer: {s['answers']['text'][0]}\"\n",
        "idx = random.sample(range(len(val_ds)), 2000) # ç¨ä¸ä¸ªæ´å°çå­éå¿«éæµè¯\n",
        "val_ds = val_ds.select(idx)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def collate(batch):\n",
        "    prompts = [fmt(x) for x in batch]\n",
        "    return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# 3) æ¿æ´» LoRA åæ¯ (è¿é¨åä¸å)\n",
        "def set_bits(cfg):\n",
        "    per_layer = cfg.get(\"per_layer_bits\", {})\n",
        "    default_b = cfg.get(\"default_w_bits\", 8)\n",
        "    activate_lora_by_bits(wrappers, per_layer, default_b)\n",
        "    return default_b\n",
        "\n",
        "# 4) åéç½®è¯ä¼° (ä¿®æ¹åå­ç»è®¡é¨å)\n",
        "@torch.no_grad()\n",
        "def eval_config(cfg, max_batches=None):\n",
        "    requantize_model_to_config(model, cfg)\n",
        "    bits = set_bits(cfg)\n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "    # --- æç¡®æå®è¦çæ§çè®¾å¤ ---\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    tot_loss, tot_tok, tot_time, seen = 0.0, 0, 0.0, 0\n",
        "\n",
        "    for i, batch in tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Eval {bits}-bit\"):\n",
        "        if (max_batches is not None) and (i >= max_batches): break\n",
        "\n",
        "        # inputs å·²ç»å¨è¿éè¢«ç§»å¨å° device\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        if device.type == 'cuda': torch.cuda.synchronize(device)\n",
        "        t0 = time.time()\n",
        "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        if device.type == 'cuda': torch.cuda.synchronize(device)\n",
        "        tot_time += (time.time() - t0)\n",
        "\n",
        "        bs = inputs[\"input_ids\"].size(0)\n",
        "        tot_loss += out.loss.item() * bs\n",
        "        tot_tok  += inputs[\"input_ids\"].numel()\n",
        "        seen     += bs\n",
        "\n",
        "    # --- å¨å¾ªç¯ç»æåï¼ä»æ­£ç¡®çè®¾å¤è¯»ååå­å³°å¼ ---\n",
        "    peak = torch.cuda.max_memory_allocated(device) if device.type == 'cuda' else 0\n",
        "    \n",
        "    avg_loss = tot_loss / max(seen, 1)\n",
        "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
        "    tps = tot_tok / max(tot_time, 1e-6)\n",
        "    mem_mb = peak / (1024**2)\n",
        "\n",
        "    # ç»éç½®ä¸ä¸ªåå­ï¼å¦æ YAML éæ²¡æçè¯\n",
        "    config_name = cfg.get(\"name\", f\"config_default_{cfg.get('default_w_bits','N/A')}\")\n",
        "    print(f\"[{config_name}] bits={bits} | loss={avg_loss:.3f} | ppl={ppl:.2f} | tokens/s={tps:.0f} | peak={mem_mb:.0f}MB\")\n",
        "    return {\"name\": config_name, \"bits\": bits, \"loss\": avg_loss, \"ppl\": ppl, \"tps\": tps, \"memMB\": mem_mb}\n",
        "\n",
        "# 5) è·ä¸¤ç§éç½® (ä¸å)\n",
        "results = []\n",
        "# ç¡®ä¿ä½ ç config_A å config_B å¨ä¹åç cell ä¸­è¢«æ­£ç¡®å®ä¹\n",
        "# æ¯å¦å¨ config.yaml å config_4bit.yaml ä¸­åå«æ·»å ä¸è¡ 'name: config_8bit' å 'name: config_4bit'\n",
        "for cfg in [config_A, config_B]:\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "    results.append(eval_config(cfg))\n",
        "\n",
        "# 6) æå°æè¡æ¦ (ä¸å)\n",
        "results.sort(key=lambda x: x[\"ppl\"])\n",
        "print(\"\\n== Leaderboard (by PPL) ==\")\n",
        "for r in results:\n",
        "    print(f\"{r['name']:>15} | ppl={r['ppl']:.2f} | loss={r['loss']:.3f} | mem={r['memMB']:.0f}MB | tps={r['tps']:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def requantize_model_to_config(model, cfg):\n",
        "    default_bits = cfg.get('default_w_bits', 8)\n",
        "    per_layer_bits = cfg.get('per_layer_bits', {})\n",
        "    \n",
        "    print(f\"ð§ Requantizing model to {default_bits}-bit...\")\n",
        "    count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, QuantLinear):\n",
        "            target_bits = per_layer_bits.get(name, default_bits)\n",
        "            old_bits = getattr(module, 'current_bits', 'unknown')\n",
        "            module.requantize_to_bits(target_bits)\n",
        "            print(f\"  {name}: {old_bits} -> {target_bits} bits\")\n",
        "            count += 1\n",
        "    print(f\"â Requantized {count} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f051902be9224c98b8a8f10b60343e7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aa6264a9df441e1ab451034fba4c298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 8-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 8-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ð§ Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "â Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            config  EM       F1  default_bits\n",
            "           C1_all8 0.0 7.120680             8\n",
            "            config 0.0 7.120680             8\n",
            "  C10_mixed_budget 0.0 5.984884             4\n",
            "   C4_back8_front4 0.0 5.849456             4\n",
            "       C5_sandwich 0.0 5.746021             4\n",
            "C8_mlpfc4_mlpproj8 0.0 5.479667             4\n",
            "   C3_front8_back4 0.0 5.426505             4\n",
            "     C6_qkv8_proj4 0.0 4.968294             4\n",
            "           C2_all4 0.0 4.829026             4\n",
            " C9_layernorm_fp32 0.0 4.829026             4\n",
            "       config_4bit 0.0 4.829026             4\n",
            "     C7_qkv4_proj8 0.0 4.296233             4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import glob, os, yaml, torch, evaluate, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.to(device); model.eval()\n",
        "metric = evaluate.load(\"squad\")\n",
        "cfg_paths = sorted(glob.glob(\"../configs/*.yaml\"))\n",
        "\n",
        "def em_f1_for_cfg(cfg):\n",
        "    requantize_model_to_config(model, cfg)\n",
        "    set_bits(cfg)\n",
        "\n",
        "    preds, refs = [], []\n",
        "    subset = val_ds.select(range(min(n, len(val_ds))))\n",
        "    for ex in tqdm(subset, leave=False):\n",
        "        prompt = f\"question: {ex['question']} context: {ex['context']} answer:\"\n",
        "        inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inp, max_new_tokens=30, pad_token_id=tokenizer.eos_token_id)\n",
        "        gen_ids = out[0, inp['input_ids'].size(1):]\n",
        "        ans = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "        preds.append({\"id\": ex[\"id\"], \"prediction_text\": ans})\n",
        "        refs.append({\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]})\n",
        "    return metric.compute(predictions=preds, references=refs)\n",
        "\n",
        "rows = []\n",
        "for p in cfg_paths:\n",
        "    with open(p, \"r\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    name = cfg.get(\"name\", os.path.basename(p).replace(\".yaml\",\"\"))\n",
        "    res = em_f1_for_cfg(cfg)\n",
        "    rows.append({\n",
        "        \"config\": name,\n",
        "        \"EM\": res[\"exact_match\"],\n",
        "        \"F1\": res[\"f1\"],\n",
        "        \"default_bits\": cfg.get(\"default_w_bits\",\"-\")\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
        "print(df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llmenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
