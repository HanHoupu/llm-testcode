{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is  going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter \n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# test generate\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=68)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   995,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "for name, mod in model.named_modules():\n",
        "    print(name, mod.__class__.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "dict_keys(['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj', 'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj', 'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj', 'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj', 'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj', 'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj', 'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj', 'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj', 'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj', 'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj', 'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj', 'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj', 'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj', 'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj', 'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj', 'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj', 'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj', 'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj', 'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj', 'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj'])\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"../configs/config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "print(cfg[\"default_w_bits\"])       \n",
        "print(cfg[\"per_layer_bits\"].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import torch.nn as nn\n",
        "\n",
        "def want_quant(name, mod, cfg):\n",
        "    # skip embedding / norm / lm_head\n",
        "    # only Linear or Conv1D\n",
        "    if name == \"lm_head\": \n",
        "        return False\n",
        "    if isinstance(mod, nn.Linear) or mod.__class__.__name__ == \"Conv1D\":\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768, 2304)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.named_modules())[10][1].weight.shape\n",
        "a,b = list(model.named_modules())[9][1].weight.shape\n",
        "a,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantLinear(nn.Module):\n",
        "    r\"\"\"Quantized version of nn.Linear.\n",
        "\n",
        "    This layer works just like nn.Linear, but the weights are stored\n",
        "    in int8 format to save memory and improve efficiency.\n",
        "\n",
        "    Instead of a single global scale, each output channel has its own\n",
        "    scale factor. This makes the quantization more accurate because\n",
        "    different output channels can have very different weight ranges.\n",
        "\n",
        "    Input shape:  (*, in_features)\n",
        "    Output shape: (*, out_features)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        bias: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.register_buffer(\"qweight\",\n",
        "            torch.empty(out_features, in_features, dtype=torch.int8, device=device))\n",
        "        self.register_buffer(\"w_scale\",\n",
        "            torch.ones(out_features, dtype=torch.float32, device=device))\n",
        "        self.register_buffer(\"w_zp\",\n",
        "            torch.zeros(out_features, dtype=torch.int32, device=device))\n",
        "        self.register_buffer(\"fp32_weight\", None)  # orignal\n",
        "        self.current_bits = None  # current \n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "    def store_fp32_weight(self, weight: torch.Tensor):\n",
        "        self.fp32_weight = weight.clone()\n",
        "\n",
        "    def requantize_to_bits(self, bits: int):\n",
        "        if self.current_bits == bits:\n",
        "            return \n",
        "        \n",
        "        self.quantize_from_float(self.fp32_weight, bits=bits)\n",
        "        self.current_bits = bits\n",
        "\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if torch.any(self.w_zp != 0):\n",
        "            # non-zero： (q - zp) * scale\n",
        "            W = (self.qweight.int() - self.w_zp.view(-1, 1)).float() * self.w_scale.view(-1, 1)\n",
        "        else:\n",
        "            # zero w zp： q * scale\n",
        "            W = self.qweight.float() * self.w_scale.view(-1, 1)\n",
        "        return F.linear(input, W, self.bias)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the extra representation of the quant module.\n",
        "        \"\"\"\n",
        "        return (f\"in_features={self.in_features}, out_features={self.out_features}, \"\n",
        "                f\"bias={self.bias is not None}, dtype=int8, per_channel=True\")\n",
        "                \n",
        "    @staticmethod\n",
        "    def get_bits_for_layer(name: str, cfg: dict) -> int:\n",
        "        return cfg[\"per_layer_bits\"][name]\n",
        "\n",
        "    def quantize_from_float(self, weight: torch.Tensor, bits: int = 8):\n",
        "        # support 2-8 bits\n",
        "        qmin, qmax = -(2**(bits-1)), 2**(bits-1) - 1 \n",
        "        w_max_abs = weight.abs().max(dim=1, keepdim=True)[0]\n",
        "        w_max_abs = torch.clamp(w_max_abs, min=1e-8)\n",
        "        scale = w_max_abs / qmax\n",
        "        qweight = torch.clamp(torch.round(weight / scale), qmin, qmax).to(torch.int8)\n",
        "        zero_point = torch.zeros(weight.size(0), dtype=torch.int32, device=weight.device)\n",
        "        self.qweight.copy_(qweight)\n",
        "        self.w_scale.copy_(scale.squeeze())\n",
        "        self.w_zp.copy_(zero_point)\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, base: nn.Linear, name: str, cfg: dict):\n",
        "        bits = cls.get_bits_for_layer(name, cfg)\n",
        "        q = cls(base.in_features, base.out_features,\n",
        "                bias=(base.bias is not None),\n",
        "                device=base.weight.device, dtype=base.weight.dtype)\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            q.store_fp32_weight(base.weight)\n",
        "\n",
        "            bits = cls.get_bits_for_layer(name, cfg)\n",
        "            q.quantize_from_float(base.weight, bits=bits)\n",
        "            q.current_bits = bits\n",
        "            \n",
        "            if base.bias is not None:\n",
        "                q.bias.copy_(base.bias)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(QuantLinear.get_bits_for_layer(\"transformer.h.0.attn.c_attn\", cfg))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def requantize_model_to_config(model, cfg):\n",
        "        \n",
        "    default_bits = cfg.get('default_w_bits', 8)\n",
        "    per_layer_bits = cfg.get('per_layer_bits', {})\n",
        "    \n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, QuantLinear):\n",
        "            target_bits = per_layer_bits.get(name, default_bits)\n",
        "            module.requantize_to_bits(target_bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "def replace_with_quant(model, cfg):\n",
        "    name_to_module = dict(model.named_modules())\n",
        "    for name, mod in list(name_to_module.items()):\n",
        "        print(name, mod.__class__.__name__)\n",
        "        if not want_quant(name, mod, cfg):\n",
        "            continue\n",
        "\n",
        "        # parent module location\n",
        "        if '.' in name:\n",
        "            parent_name, child_name = name.rsplit('.', 1)\n",
        "            parent = name_to_module[parent_name]\n",
        "        else:\n",
        "            parent, child_name = model, name\n",
        "\n",
        "        # convert Conv1D to Linear\n",
        "        if mod.__class__.__name__ == \"Conv1D\":\n",
        "                in_f, out_f = mod.weight.shape          # Conv1D weight is (out, in)\n",
        "                base = nn.Linear(in_f, out_f, bias=(mod.bias is not None))\n",
        "                base.to(mod.weight.device, dtype=mod.weight.dtype)\n",
        "                with torch.no_grad():\n",
        "                    base.weight.copy_(mod.weight.T)       # copy \n",
        "                    if mod.bias is not None:\n",
        "                        base.bias.copy_(mod.bias)\n",
        "        else:\n",
        "            base = mod\n",
        "\n",
        "        # construct quantization wrapper\n",
        "        qcfg = cfg.copy()\n",
        "        qmod = QuantLinear.from_linear(base, name,cfg=qcfg)\n",
        "\n",
        "        # replace the original layer with setattr\n",
        "        setattr(parent, child_name, qmod)\n",
        "\n",
        "replace_with_quant(model, cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn Conv1D\n",
            "transformer.h.0.attn.c_proj Conv1D\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc Conv1D\n",
            "transformer.h.0.mlp.c_proj Conv1D\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn Conv1D\n",
            "transformer.h.1.attn.c_proj Conv1D\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc Conv1D\n",
            "transformer.h.1.mlp.c_proj Conv1D\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn Conv1D\n",
            "transformer.h.2.attn.c_proj Conv1D\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc Conv1D\n",
            "transformer.h.2.mlp.c_proj Conv1D\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn Conv1D\n",
            "transformer.h.3.attn.c_proj Conv1D\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc Conv1D\n",
            "transformer.h.3.mlp.c_proj Conv1D\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn Conv1D\n",
            "transformer.h.4.attn.c_proj Conv1D\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc Conv1D\n",
            "transformer.h.4.mlp.c_proj Conv1D\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn Conv1D\n",
            "transformer.h.5.attn.c_proj Conv1D\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc Conv1D\n",
            "transformer.h.5.mlp.c_proj Conv1D\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn Conv1D\n",
            "transformer.h.6.attn.c_proj Conv1D\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc Conv1D\n",
            "transformer.h.6.mlp.c_proj Conv1D\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn Conv1D\n",
            "transformer.h.7.attn.c_proj Conv1D\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc Conv1D\n",
            "transformer.h.7.mlp.c_proj Conv1D\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn Conv1D\n",
            "transformer.h.8.attn.c_proj Conv1D\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc Conv1D\n",
            "transformer.h.8.mlp.c_proj Conv1D\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn Conv1D\n",
            "transformer.h.9.attn.c_proj Conv1D\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc Conv1D\n",
            "transformer.h.9.mlp.c_proj Conv1D\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn Conv1D\n",
            "transformer.h.10.attn.c_proj Conv1D\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc Conv1D\n",
            "transformer.h.10.mlp.c_proj Conv1D\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn Conv1D\n",
            "transformer.h.11.attn.c_proj Conv1D\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc Conv1D\n",
            "transformer.h.11.mlp.c_proj Conv1D\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n",
            "Result: Hello, world is  in the world. \n",
            "I've been in the a small small small small small\n"
          ]
        }
      ],
      "source": [
        "# Step 1 test - fresh model with original methods\n",
        "test_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "replace_with_quant(test_model, cfg)\n",
        "\n",
        "# Test 2bit/6bit requantization  \n",
        "with open(\"../configs/test_2bit_6bit.yaml\", \"r\") as f:\n",
        "    test_cfg = yaml.safe_load(f)\n",
        "requantize_model_to_config(test_model, test_cfg)\n",
        "\n",
        "# Generation test\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = test_model.generate(**inputs, max_length=30)\n",
        "print(\"Result:\", tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world is  going to be a lot more interesting than it was before.\n",
            "I'm not sure if I'm going to be able to do this, but I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "I'm going to be able to do it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "inputs = tokenizer(\"Hello, world is \", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=68)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, in_f, out_f, r=4, alpha=None):\n",
        "        super().__init__()\n",
        "        self.scale = (alpha or r) / r\n",
        "        self.A = nn.Parameter(torch.randn(r, in_f) * 0.01)\n",
        "        self.B = nn.Parameter(torch.zeros(out_f, r))\n",
        "    def forward(self, x):\n",
        "        return (x @ self.A.t()) @ self.B.t() * self.scale\n",
        "\n",
        "class LoRAWrapped(nn.Module):\n",
        "    def __init__(self, base, branches, layer_name=None):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # keep device and precision\n",
        "        dev = next(self.base.parameters()).device\n",
        "        dtype = next(self.base.parameters()).dtype\n",
        "\n",
        "        in_f = self.base.in_features\n",
        "        out_f = self.base.out_features\n",
        "\n",
        "        self.bank = nn.ModuleDict({\n",
        "            k: LoRA(in_f, out_f, r, a).to(device=dev, dtype=dtype)\n",
        "            for k, (r, a) in branches.items()\n",
        "        })\n",
        "        self.active = None            # only one branch\n",
        "        self.layer_name = layer_name  # for logging / routing (optional)\n",
        "\n",
        "    def set_active(self, name_or_none):\n",
        "        self.active = name_or_none   # name_or_none: 'bw4' / 'bw8' / None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.base(x)\n",
        "        if self.active in self.bank:\n",
        "            y = y + self.bank[self.active](x)\n",
        "        return y\n",
        "\n",
        "def attach_lora_to_quant(model, name2branches, quant_cfg):\n",
        "    \"\"\"\n",
        "    Lora and quant at the same time\n",
        "    \"\"\"\n",
        "    # quant\n",
        "    replace_with_quant(model, quant_cfg)\n",
        "    \n",
        "    # add LoRA\n",
        "    wrappers = {}\n",
        "    for name, mod in list(model.named_modules()):\n",
        "        if name in name2branches and hasattr(mod, 'in_features'):\n",
        "            parent = model.get_submodule(name.rsplit('.',1)[0]) if '.' in name else model\n",
        "            attr = name.split('.')[-1]\n",
        "            w = LoRAWrapped(mod, name2branches[name], layer_name=name)\n",
        "            setattr(parent, attr, w)\n",
        "            wrappers[name] = w\n",
        "    return wrappers\n",
        "\n",
        "# activate by bit config (call this before inference)\n",
        "def activate_lora_by_bits(wrappers, bit_cfg, default_bits=None):\n",
        "    m = {4: \"bw4\", 8: \"bw8\"}  # 4/8\n",
        "    for n, w in wrappers.items():\n",
        "        bw = bit_cfg.get(n, default_bits)\n",
        "        w.set_active(m[bw])  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT2LMHeadModel\n",
            "transformer GPT2Model\n",
            "transformer.wte Embedding\n",
            "transformer.wpe Embedding\n",
            "transformer.drop Dropout\n",
            "transformer.h ModuleList\n",
            "transformer.h.0 GPT2Block\n",
            "transformer.h.0.ln_1 LayerNorm\n",
            "transformer.h.0.attn GPT2Attention\n",
            "transformer.h.0.attn.c_attn QuantLinear\n",
            "transformer.h.0.attn.c_proj QuantLinear\n",
            "transformer.h.0.attn.attn_dropout Dropout\n",
            "transformer.h.0.attn.resid_dropout Dropout\n",
            "transformer.h.0.ln_2 LayerNorm\n",
            "transformer.h.0.mlp GPT2MLP\n",
            "transformer.h.0.mlp.c_fc QuantLinear\n",
            "transformer.h.0.mlp.c_proj QuantLinear\n",
            "transformer.h.0.mlp.act NewGELUActivation\n",
            "transformer.h.0.mlp.dropout Dropout\n",
            "transformer.h.1 GPT2Block\n",
            "transformer.h.1.ln_1 LayerNorm\n",
            "transformer.h.1.attn GPT2Attention\n",
            "transformer.h.1.attn.c_attn QuantLinear\n",
            "transformer.h.1.attn.c_proj QuantLinear\n",
            "transformer.h.1.attn.attn_dropout Dropout\n",
            "transformer.h.1.attn.resid_dropout Dropout\n",
            "transformer.h.1.ln_2 LayerNorm\n",
            "transformer.h.1.mlp GPT2MLP\n",
            "transformer.h.1.mlp.c_fc QuantLinear\n",
            "transformer.h.1.mlp.c_proj QuantLinear\n",
            "transformer.h.1.mlp.act NewGELUActivation\n",
            "transformer.h.1.mlp.dropout Dropout\n",
            "transformer.h.2 GPT2Block\n",
            "transformer.h.2.ln_1 LayerNorm\n",
            "transformer.h.2.attn GPT2Attention\n",
            "transformer.h.2.attn.c_attn QuantLinear\n",
            "transformer.h.2.attn.c_proj QuantLinear\n",
            "transformer.h.2.attn.attn_dropout Dropout\n",
            "transformer.h.2.attn.resid_dropout Dropout\n",
            "transformer.h.2.ln_2 LayerNorm\n",
            "transformer.h.2.mlp GPT2MLP\n",
            "transformer.h.2.mlp.c_fc QuantLinear\n",
            "transformer.h.2.mlp.c_proj QuantLinear\n",
            "transformer.h.2.mlp.act NewGELUActivation\n",
            "transformer.h.2.mlp.dropout Dropout\n",
            "transformer.h.3 GPT2Block\n",
            "transformer.h.3.ln_1 LayerNorm\n",
            "transformer.h.3.attn GPT2Attention\n",
            "transformer.h.3.attn.c_attn QuantLinear\n",
            "transformer.h.3.attn.c_proj QuantLinear\n",
            "transformer.h.3.attn.attn_dropout Dropout\n",
            "transformer.h.3.attn.resid_dropout Dropout\n",
            "transformer.h.3.ln_2 LayerNorm\n",
            "transformer.h.3.mlp GPT2MLP\n",
            "transformer.h.3.mlp.c_fc QuantLinear\n",
            "transformer.h.3.mlp.c_proj QuantLinear\n",
            "transformer.h.3.mlp.act NewGELUActivation\n",
            "transformer.h.3.mlp.dropout Dropout\n",
            "transformer.h.4 GPT2Block\n",
            "transformer.h.4.ln_1 LayerNorm\n",
            "transformer.h.4.attn GPT2Attention\n",
            "transformer.h.4.attn.c_attn QuantLinear\n",
            "transformer.h.4.attn.c_proj QuantLinear\n",
            "transformer.h.4.attn.attn_dropout Dropout\n",
            "transformer.h.4.attn.resid_dropout Dropout\n",
            "transformer.h.4.ln_2 LayerNorm\n",
            "transformer.h.4.mlp GPT2MLP\n",
            "transformer.h.4.mlp.c_fc QuantLinear\n",
            "transformer.h.4.mlp.c_proj QuantLinear\n",
            "transformer.h.4.mlp.act NewGELUActivation\n",
            "transformer.h.4.mlp.dropout Dropout\n",
            "transformer.h.5 GPT2Block\n",
            "transformer.h.5.ln_1 LayerNorm\n",
            "transformer.h.5.attn GPT2Attention\n",
            "transformer.h.5.attn.c_attn QuantLinear\n",
            "transformer.h.5.attn.c_proj QuantLinear\n",
            "transformer.h.5.attn.attn_dropout Dropout\n",
            "transformer.h.5.attn.resid_dropout Dropout\n",
            "transformer.h.5.ln_2 LayerNorm\n",
            "transformer.h.5.mlp GPT2MLP\n",
            "transformer.h.5.mlp.c_fc QuantLinear\n",
            "transformer.h.5.mlp.c_proj QuantLinear\n",
            "transformer.h.5.mlp.act NewGELUActivation\n",
            "transformer.h.5.mlp.dropout Dropout\n",
            "transformer.h.6 GPT2Block\n",
            "transformer.h.6.ln_1 LayerNorm\n",
            "transformer.h.6.attn GPT2Attention\n",
            "transformer.h.6.attn.c_attn QuantLinear\n",
            "transformer.h.6.attn.c_proj QuantLinear\n",
            "transformer.h.6.attn.attn_dropout Dropout\n",
            "transformer.h.6.attn.resid_dropout Dropout\n",
            "transformer.h.6.ln_2 LayerNorm\n",
            "transformer.h.6.mlp GPT2MLP\n",
            "transformer.h.6.mlp.c_fc QuantLinear\n",
            "transformer.h.6.mlp.c_proj QuantLinear\n",
            "transformer.h.6.mlp.act NewGELUActivation\n",
            "transformer.h.6.mlp.dropout Dropout\n",
            "transformer.h.7 GPT2Block\n",
            "transformer.h.7.ln_1 LayerNorm\n",
            "transformer.h.7.attn GPT2Attention\n",
            "transformer.h.7.attn.c_attn QuantLinear\n",
            "transformer.h.7.attn.c_proj QuantLinear\n",
            "transformer.h.7.attn.attn_dropout Dropout\n",
            "transformer.h.7.attn.resid_dropout Dropout\n",
            "transformer.h.7.ln_2 LayerNorm\n",
            "transformer.h.7.mlp GPT2MLP\n",
            "transformer.h.7.mlp.c_fc QuantLinear\n",
            "transformer.h.7.mlp.c_proj QuantLinear\n",
            "transformer.h.7.mlp.act NewGELUActivation\n",
            "transformer.h.7.mlp.dropout Dropout\n",
            "transformer.h.8 GPT2Block\n",
            "transformer.h.8.ln_1 LayerNorm\n",
            "transformer.h.8.attn GPT2Attention\n",
            "transformer.h.8.attn.c_attn QuantLinear\n",
            "transformer.h.8.attn.c_proj QuantLinear\n",
            "transformer.h.8.attn.attn_dropout Dropout\n",
            "transformer.h.8.attn.resid_dropout Dropout\n",
            "transformer.h.8.ln_2 LayerNorm\n",
            "transformer.h.8.mlp GPT2MLP\n",
            "transformer.h.8.mlp.c_fc QuantLinear\n",
            "transformer.h.8.mlp.c_proj QuantLinear\n",
            "transformer.h.8.mlp.act NewGELUActivation\n",
            "transformer.h.8.mlp.dropout Dropout\n",
            "transformer.h.9 GPT2Block\n",
            "transformer.h.9.ln_1 LayerNorm\n",
            "transformer.h.9.attn GPT2Attention\n",
            "transformer.h.9.attn.c_attn QuantLinear\n",
            "transformer.h.9.attn.c_proj QuantLinear\n",
            "transformer.h.9.attn.attn_dropout Dropout\n",
            "transformer.h.9.attn.resid_dropout Dropout\n",
            "transformer.h.9.ln_2 LayerNorm\n",
            "transformer.h.9.mlp GPT2MLP\n",
            "transformer.h.9.mlp.c_fc QuantLinear\n",
            "transformer.h.9.mlp.c_proj QuantLinear\n",
            "transformer.h.9.mlp.act NewGELUActivation\n",
            "transformer.h.9.mlp.dropout Dropout\n",
            "transformer.h.10 GPT2Block\n",
            "transformer.h.10.ln_1 LayerNorm\n",
            "transformer.h.10.attn GPT2Attention\n",
            "transformer.h.10.attn.c_attn QuantLinear\n",
            "transformer.h.10.attn.c_proj QuantLinear\n",
            "transformer.h.10.attn.attn_dropout Dropout\n",
            "transformer.h.10.attn.resid_dropout Dropout\n",
            "transformer.h.10.ln_2 LayerNorm\n",
            "transformer.h.10.mlp GPT2MLP\n",
            "transformer.h.10.mlp.c_fc QuantLinear\n",
            "transformer.h.10.mlp.c_proj QuantLinear\n",
            "transformer.h.10.mlp.act NewGELUActivation\n",
            "transformer.h.10.mlp.dropout Dropout\n",
            "transformer.h.11 GPT2Block\n",
            "transformer.h.11.ln_1 LayerNorm\n",
            "transformer.h.11.attn GPT2Attention\n",
            "transformer.h.11.attn.c_attn QuantLinear\n",
            "transformer.h.11.attn.c_proj QuantLinear\n",
            "transformer.h.11.attn.attn_dropout Dropout\n",
            "transformer.h.11.attn.resid_dropout Dropout\n",
            "transformer.h.11.ln_2 LayerNorm\n",
            "transformer.h.11.mlp GPT2MLP\n",
            "transformer.h.11.mlp.c_fc QuantLinear\n",
            "transformer.h.11.mlp.c_proj QuantLinear\n",
            "transformer.h.11.mlp.act NewGELUActivation\n",
            "transformer.h.11.mlp.dropout Dropout\n",
            "transformer.ln_f LayerNorm\n",
            "lm_head Linear\n"
          ]
        }
      ],
      "source": [
        "# define the quantization bit width and LoRA branches for each layer\n",
        "lora_spec = {\n",
        "    \"transformer.h.0.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    \"transformer.h.1.attn.c_attn\": {\"bw4\": (8,16), \"bw8\": (4,8)},\n",
        "    # ... more layers\n",
        "}\n",
        "\n",
        "# one-step: quant + LoRA\n",
        "wrappers = attach_lora_to_quant(model, lora_spec, cfg)\n",
        "\n",
        "# \n",
        "for name, wrapper in wrappers.items():\n",
        "    bits = cfg[\"per_layer_bits\"].get(name, cfg[\"default_w_bits\"])\n",
        "    if bits <= 4:\n",
        "        wrapper.set_active(\"bw4\")\n",
        "    else:   \n",
        "        wrapper.set_active(\"bw8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "squad_dataset = load_dataset(\"squad\", split=\"train\")\n",
        "\n",
        "# Format dataset to prompt\n",
        "def format_squad_prompt(sample):\n",
        "    return f\"question: {sample['question']} context: {sample['context']} answer: {sample['answers']['text'][0]}\"\n",
        "\n",
        "# Create a small subset\n",
        "subset_indices = random.sample(range(len(squad_dataset)), 1000)\n",
        "squad_subset = squad_dataset.select(subset_indices)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "# DataLoader\n",
        "def collate_fn(batch):\n",
        "    prompts = [format_squad_prompt(s) for s in batch]\n",
        "    return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "train_dataloader = DataLoader(squad_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load two config files directly into a list\n",
        "with open(\"../configs/config.yaml\", 'r') as f: config_A = yaml.safe_load(f)\n",
        "with open(\"../configs/config_4bit.yaml\", 'r') as f: config_B = yaml.safe_load(f)\n",
        "\n",
        "precision_configs = [config_A, config_B]\n",
        "\n",
        "# Collect all LoRA module parameters\n",
        "lora_params = [\n",
        "    p for w in wrappers.values() for p in w.bank.parameters() if p.requires_grad\n",
        "]\n",
        "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "Iteration 1000 | Loss: 3.854 | Config: 8-bit: 100%|██████████| 1000/1000 [00:43<00:00, 22.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.train()  # Set to training mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Use tqdm to create a progress bar\n",
        "progress_bar = tqdm(range(1000))\n",
        "\n",
        "# Get data from dataloader\n",
        "data_iter = iter(train_dataloader)\n",
        "\n",
        "for i in progress_bar:\n",
        "    # If data is used up, create a new iterator\n",
        "    try:\n",
        "        batch = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(train_dataloader)\n",
        "        batch = next(data_iter)\n",
        "\n",
        "    # Randomly select a precision configuration\n",
        "    chosen_config = random.choice(precision_configs)\n",
        "\n",
        "    # Activate the corresponding LoRA branch based on the selected configuration\n",
        "    per_layer_config = chosen_config.get('per_layer_bits', {})\n",
        "    default_bits = chosen_config.get('default_w_bits')\n",
        "    activate_lora_by_bits(wrappers, per_layer_config, default_bits)\n",
        "\n",
        "    #training process\n",
        "    inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Update the progress bar display\n",
        "    progress_bar.set_description(f\"Iteration {i+1} | Loss: {loss.item():.3f} | Config: {chosen_config['default_w_bits']}-bit\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[A] default8 => France is a country in Western Europe. Its capital and largest city is Paris. Its capital and largest city is Paris. Its capital and largest city is Paris.\n",
            "[B] default4 => France is a country in Western Europe. Its capital and largest city is Paris. Its capital and largest city is Paris. Its capital and largest city is Paris.\n"
          ]
        }
      ],
      "source": [
        "# --- Quick smoke test for 2 configs ---\n",
        "model.eval()\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "q = \"What is the capital of France?\"\n",
        "c = \"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
        "prompt = f\"question: {q} context: {c} answer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_with(cfg):\n",
        "    activate_lora_by_bits(wrappers, cfg.get('per_layer_bits', {}), cfg.get('default_w_bits'))\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=32, do_sample=False,  # 更稳定\n",
        "        pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    gen = out[0, inputs['input_ids'].shape[1]:]      # 只取新生成部分\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"[A]\", cfg_A_name := config_A.get(\"name\", f\"default{config_A.get('default_w_bits')}\"), \"=>\", gen_with(config_A))\n",
        "print(\"[B]\", cfg_B_name := config_B.get(\"name\", f\"default{config_B.get('default_w_bits')}\"), \"=>\", gen_with(config_B))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "评估将使用设备: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval 8-bit: 100%|██████████| 250/250 [00:09<00:00, 26.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config_default_8] bits=8 | loss=3.468 | ppl=32.06 | tokens/s=35365 | peak=1620MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval 4-bit: 100%|██████████| 250/250 [00:09<00:00, 27.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[config_default_4] bits=4 | loss=3.719 | ppl=41.22 | tokens/s=35353 | peak=1620MB\n",
            "\n",
            "== Leaderboard (by PPL) ==\n",
            "config_default_8 | ppl=32.06 | loss=3.468 | mem=1620MB | tps=35365\n",
            "config_default_4 | ppl=41.22 | loss=3.719 | mem=1620MB | tps=35353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# cell 16 (修改后)\n",
        "import math, time, torch, random, gc\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. 在 Cell 的最开始就定义好全局设备 ---\n",
        "# 这样后续所有操作都会默认使用这个设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"评估将使用设备: {device}\")\n",
        "\n",
        "model.to(device) # 确保模型在正确的设备上\n",
        "model.eval()\n",
        "\n",
        "# 2) 准备验证集 (这部分不变)\n",
        "val_ds = load_dataset(\"squad\", split=\"validation\")\n",
        "def fmt(s): return f\"question: {s['question']} context: {s['context']} answer: {s['answers']['text'][0]}\"\n",
        "idx = random.sample(range(len(val_ds)), 2000) # 用一个更小的子集快速测试\n",
        "val_ds = val_ds.select(idx)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def collate(batch):\n",
        "    prompts = [fmt(x) for x in batch]\n",
        "    return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# 3) 激活 LoRA 分支 (这部分不变)\n",
        "def set_bits(cfg):\n",
        "    per_layer = cfg.get(\"per_layer_bits\", {})\n",
        "    default_b = cfg.get(\"default_w_bits\", 8)\n",
        "    activate_lora_by_bits(wrappers, per_layer, default_b)\n",
        "    return default_b\n",
        "\n",
        "# 4) 单配置评估 (修改内存统计部分)\n",
        "@torch.no_grad()\n",
        "def eval_config(cfg, max_batches=None):\n",
        "    requantize_model_to_config(model, cfg)\n",
        "    bits = set_bits(cfg)\n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "    # --- 明确指定要监控的设备 ---\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    tot_loss, tot_tok, tot_time, seen = 0.0, 0, 0.0, 0\n",
        "\n",
        "    for i, batch in tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Eval {bits}-bit\"):\n",
        "        if (max_batches is not None) and (i >= max_batches): break\n",
        "\n",
        "        # inputs 已经在这里被移动到 device\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        if device.type == 'cuda': torch.cuda.synchronize(device)\n",
        "        t0 = time.time()\n",
        "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        if device.type == 'cuda': torch.cuda.synchronize(device)\n",
        "        tot_time += (time.time() - t0)\n",
        "\n",
        "        bs = inputs[\"input_ids\"].size(0)\n",
        "        tot_loss += out.loss.item() * bs\n",
        "        tot_tok  += inputs[\"input_ids\"].numel()\n",
        "        seen     += bs\n",
        "\n",
        "    # --- 在循环结束后，从正确的设备读取内存峰值 ---\n",
        "    peak = torch.cuda.max_memory_allocated(device) if device.type == 'cuda' else 0\n",
        "    \n",
        "    avg_loss = tot_loss / max(seen, 1)\n",
        "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
        "    tps = tot_tok / max(tot_time, 1e-6)\n",
        "    mem_mb = peak / (1024**2)\n",
        "\n",
        "    # 给配置一个名字，如果 YAML 里没有的话\n",
        "    config_name = cfg.get(\"name\", f\"config_default_{cfg.get('default_w_bits','N/A')}\")\n",
        "    print(f\"[{config_name}] bits={bits} | loss={avg_loss:.3f} | ppl={ppl:.2f} | tokens/s={tps:.0f} | peak={mem_mb:.0f}MB\")\n",
        "    return {\"name\": config_name, \"bits\": bits, \"loss\": avg_loss, \"ppl\": ppl, \"tps\": tps, \"memMB\": mem_mb}\n",
        "\n",
        "# 5) 跑两种配置 (不变)\n",
        "results = []\n",
        "# 确保你的 config_A 和 config_B 在之前的 cell 中被正确定义\n",
        "# 比如在 config.yaml 和 config_4bit.yaml 中分别添加一行 'name: config_8bit' 和 'name: config_4bit'\n",
        "for cfg in [config_A, config_B]:\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "    results.append(eval_config(cfg))\n",
        "\n",
        "# 6) 打印排行榜 (不变)\n",
        "results.sort(key=lambda x: x[\"ppl\"])\n",
        "print(\"\\n== Leaderboard (by PPL) ==\")\n",
        "for r in results:\n",
        "    print(f\"{r['name']:>15} | ppl={r['ppl']:.2f} | loss={r['loss']:.3f} | mem={r['memMB']:.0f}MB | tps={r['tps']:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def requantize_model_to_config(model, cfg):\n",
        "    default_bits = cfg.get('default_w_bits', 8)\n",
        "    per_layer_bits = cfg.get('per_layer_bits', {})\n",
        "    \n",
        "    print(f\"🔧 Requantizing model to {default_bits}-bit...\")\n",
        "    count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, QuantLinear):\n",
        "            target_bits = per_layer_bits.get(name, default_bits)\n",
        "            old_bits = getattr(module, 'current_bits', 'unknown')\n",
        "            module.requantize_to_bits(target_bits)\n",
        "            print(f\"  {name}: {old_bits} -> {target_bits} bits\")\n",
        "            count += 1\n",
        "    print(f\"✅ Requantized {count} layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f051902be9224c98b8a8f10b60343e7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aa6264a9df441e1ab451034fba4c298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 8-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 8-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.0.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.0.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_attn.base: 4 -> 8 bits\n",
            "  transformer.h.1.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.1.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.2.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.2.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.3.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.3.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.4.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.4.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.5.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.5.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.6.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.6.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.7.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.7.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.8.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.8.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.9.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.9.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.10.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.10.mlp.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_attn: 4 -> 8 bits\n",
            "  transformer.h.11.attn.c_proj: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_fc: 4 -> 8 bits\n",
            "  transformer.h.11.mlp.c_proj: 4 -> 8 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Requantizing model to 4-bit...\n",
            "  transformer.h.0.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.0.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.0.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_attn.base: 8 -> 4 bits\n",
            "  transformer.h.1.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.1.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.2.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.2.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.3.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.3.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.4.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.4.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.5.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.5.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.6.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.6.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.7.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.7.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.8.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.8.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.9.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.9.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.10.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.10.mlp.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_attn: 8 -> 4 bits\n",
            "  transformer.h.11.attn.c_proj: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_fc: 8 -> 4 bits\n",
            "  transformer.h.11.mlp.c_proj: 8 -> 4 bits\n",
            "✅ Requantized 48 layers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            config  EM       F1  default_bits\n",
            "           C1_all8 0.0 7.120680             8\n",
            "            config 0.0 7.120680             8\n",
            "  C10_mixed_budget 0.0 5.984884             4\n",
            "   C4_back8_front4 0.0 5.849456             4\n",
            "       C5_sandwich 0.0 5.746021             4\n",
            "C8_mlpfc4_mlpproj8 0.0 5.479667             4\n",
            "   C3_front8_back4 0.0 5.426505             4\n",
            "     C6_qkv8_proj4 0.0 4.968294             4\n",
            "           C2_all4 0.0 4.829026             4\n",
            " C9_layernorm_fp32 0.0 4.829026             4\n",
            "       config_4bit 0.0 4.829026             4\n",
            "     C7_qkv4_proj8 0.0 4.296233             4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import glob, os, yaml, torch, evaluate, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.to(device); model.eval()\n",
        "metric = evaluate.load(\"squad\")\n",
        "cfg_paths = sorted(glob.glob(\"../configs/*.yaml\"))\n",
        "\n",
        "def em_f1_for_cfg(cfg):\n",
        "    requantize_model_to_config(model, cfg)\n",
        "    set_bits(cfg)\n",
        "\n",
        "    preds, refs = [], []\n",
        "    subset = val_ds.select(range(min(n, len(val_ds))))\n",
        "    for ex in tqdm(subset, leave=False):\n",
        "        prompt = f\"question: {ex['question']} context: {ex['context']} answer:\"\n",
        "        inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inp, max_new_tokens=30, pad_token_id=tokenizer.eos_token_id)\n",
        "        gen_ids = out[0, inp['input_ids'].size(1):]\n",
        "        ans = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "        preds.append({\"id\": ex[\"id\"], \"prediction_text\": ans})\n",
        "        refs.append({\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]})\n",
        "    return metric.compute(predictions=preds, references=refs)\n",
        "\n",
        "rows = []\n",
        "for p in cfg_paths:\n",
        "    with open(p, \"r\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    name = cfg.get(\"name\", os.path.basename(p).replace(\".yaml\",\"\"))\n",
        "    res = em_f1_for_cfg(cfg)\n",
        "    rows.append({\n",
        "        \"config\": name,\n",
        "        \"EM\": res[\"exact_match\"],\n",
        "        \"F1\": res[\"f1\"],\n",
        "        \"default_bits\": cfg.get(\"default_w_bits\",\"-\")\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
        "print(df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llmenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
